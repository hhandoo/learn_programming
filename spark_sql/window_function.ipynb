{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run tools.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "SELECT\n",
    "    Employee,\n",
    "    Department,\n",
    "    Sales,\n",
    "    DENSE_RANK() OVER (PARTITION BY Department ORDER BY Sales DESC) AS Rank\n",
    "FROM employee_sales;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"age\").orderBy(col(\"key\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+-------------+----------+----+---+\n",
      "|key|first_name|last_name|age|         city|dense_rank|Rank|sum|\n",
      "+---+----------+---------+---+-------------+----------+----+---+\n",
      "| 41|     Syzmr|  Veqyann| 18|  Los Angeles|         1|   1| 18|\n",
      "| 31|     Nqwlo|  Eblxujd| 18|     New York|         2|   2| 36|\n",
      "| 47|     Lhzsl|  Ucykpsb| 19|        Miami|         1|   1| 19|\n",
      "| 30|     Omhcr|  Mgjoyqa| 19|       Boston|         2|   2| 38|\n",
      "| 11|     Rbynr|  Wivyegj| 19|      Chicago|         3|   3| 57|\n",
      "| 93|     Hzycx|  Rpgoexz| 20|      Seattle|         1|   1| 20|\n",
      "| 78|     Hbbcb|  Cqthngf| 20|      Seattle|         2|   2| 40|\n",
      "| 63|     Urlnz|  Oekczoy| 20|       Boston|         3|   3| 60|\n",
      "| 54|     Afszn|  Ymsskrw| 20|       Boston|         4|   4| 80|\n",
      "| 46|     Qxwjh|  Cwmazca| 20|  Los Angeles|         5|   5|100|\n",
      "| 34|     Wqull|  Ogxknus| 20|        Miami|         6|   6|120|\n",
      "| 75|     Mebup|  Nampubi| 22|      Chicago|         1|   1| 22|\n",
      "| 21|     Aznkq|  Ovqzdmt| 22|      Houston|         2|   2| 44|\n",
      "| 62|     Uxxey|  Iwzpezb| 23|San Francisco|         1|   1| 23|\n",
      "| 39|     Xwfew|  Bqqrbxp| 25|       Boston|         1|   1| 25|\n",
      "| 87|     Abppp|  Vptghmv| 26|San Francisco|         1|   1| 26|\n",
      "| 16|     Aqrql|  Wjndtfc| 26|  Los Angeles|         2|   2| 52|\n",
      "|  5|     Atzue|  Xrhypyt| 26|     New York|         3|   3| 78|\n",
      "| 15|     Nscof|  Ejkbpev| 27|     New York|         1|   1| 27|\n",
      "|  1|     Uzwsp|  Xmeejci| 27|        Miami|         2|   2| 54|\n",
      "+---+----------+---------+---+-------------+----------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ranked = df.withColumn(\"dense_rank\", dense_rank().over(windowSpec))\\\n",
    ".withColumn(\"Rank\", rank().over(windowSpec))\\\n",
    ".withColumn(\"sum\", sum(col(\"age\")).over(windowSpec))\n",
    "\n",
    "# Show Output\n",
    "df_ranked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario: Advanced Sales Analysis\n",
    "We have an employee_sales dataset with columns:\n",
    "\n",
    "Employee\n",
    "Department\n",
    "Month\n",
    "Sales\n",
    "Objectives\n",
    "Rank employees by Sales within each Department (DENSE_RANK)\n",
    "Calculate a Running Total of Sales for each Department (SUM)\n",
    "Find the Difference Between an Employee’s Sales and the Department’s Average Sales (AVG)\n",
    "Get the Previous Month's Sales for Each Employee (LAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "SELECT\n",
    "    Employee,\n",
    "    Department,\n",
    "    Month,\n",
    "    Sales,\n",
    "    DENSE_RANK() OVER (PARTITION BY Department ORDER BY Sales DESC) AS Rank,\n",
    "    SUM(Sales) OVER (PARTITION BY Department ORDER BY Month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS RunningTotalSales,\n",
    "    AVG(Sales) OVER (PARTITION BY Department) AS DeptAvgSales,\n",
    "    LAG(Sales, 1) OVER (PARTITION BY Employee ORDER BY Month) AS PreviousMonthSales\n",
    "FROM employee_sales;\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+-----+\n",
      "|Employee|Department|Month|Sales|\n",
      "+--------+----------+-----+-----+\n",
      "|   Alice|        IT|  Jan| 5000|\n",
      "|     Bob|        IT|  Jan| 7000|\n",
      "| Charlie|        IT|  Jan| 6000|\n",
      "|   Alice|        IT|  Feb| 6500|\n",
      "|     Bob|        IT|  Feb| 8000|\n",
      "| Charlie|        IT|  Feb| 7500|\n",
      "|   David|        HR|  Jan| 4000|\n",
      "|     Eve|        HR|  Jan| 4500|\n",
      "|   David|        HR|  Feb| 4800|\n",
      "|     Eve|        HR|  Feb| 5300|\n",
      "+--------+----------+-----+-----+\n",
      "\n",
      "+--------+----------+-----+-----+----+-----------------+-----------------+------------------+\n",
      "|Employee|Department|Month|Sales|Rank|RunningTotalSales|     DeptAvgSales|PreviousMonthSales|\n",
      "+--------+----------+-----+-----+----+-----------------+-----------------+------------------+\n",
      "|   Alice|        IT|  Feb| 6500|   4|            22000|6666.666666666667|              NULL|\n",
      "|   Alice|        IT|  Jan| 5000|   6|            40000|6666.666666666667|              6500|\n",
      "|     Bob|        IT|  Feb| 8000|   1|             8000|6666.666666666667|              NULL|\n",
      "|     Bob|        IT|  Jan| 7000|   3|            29000|6666.666666666667|              8000|\n",
      "| Charlie|        IT|  Feb| 7500|   2|            15500|6666.666666666667|              NULL|\n",
      "| Charlie|        IT|  Jan| 6000|   5|            35000|6666.666666666667|              7500|\n",
      "|   David|        HR|  Feb| 4800|   2|            10100|           4650.0|              NULL|\n",
      "|   David|        HR|  Jan| 4000|   4|            18600|           4650.0|              4800|\n",
      "|     Eve|        HR|  Feb| 5300|   1|             5300|           4650.0|              NULL|\n",
      "|     Eve|        HR|  Jan| 4500|   3|            14600|           4650.0|              5300|\n",
      "+--------+----------+-----+-----+----+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, dense_rank, sum, avg, lag\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"ComplexWindowFunctions\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (\"Alice\", \"IT\", \"Jan\", 5000),\n",
    "    (\"Bob\", \"IT\", \"Jan\", 7000),\n",
    "    (\"Charlie\", \"IT\", \"Jan\", 6000),\n",
    "    (\"Alice\", \"IT\", \"Feb\", 6500),\n",
    "    (\"Bob\", \"IT\", \"Feb\", 8000),\n",
    "    (\"Charlie\", \"IT\", \"Feb\", 7500),\n",
    "    (\"David\", \"HR\", \"Jan\", 4000),\n",
    "    (\"Eve\", \"HR\", \"Jan\", 4500),\n",
    "    (\"David\", \"HR\", \"Feb\", 4800),\n",
    "    (\"Eve\", \"HR\", \"Feb\", 5300),\n",
    "]\n",
    "\n",
    "# Define Schema\n",
    "columns = [\"Employee\", \"Department\", \"Month\", \"Sales\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Define Window Specifications\n",
    "dept_window = Window.partitionBy(\"Department\").orderBy(\n",
    "    col(\"Sales\").desc()\n",
    ")  # For ranking\n",
    "running_total_window = (\n",
    "    Window.partitionBy(\"Department\")\n",
    "    .orderBy(\"Month\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")  # Running total\n",
    "dept_avg_window = Window.partitionBy(\"Department\")  # Department-wise average\n",
    "previous_sales_window = Window.partitionBy(\"Employee\").orderBy(\n",
    "    \"Month\"\n",
    ")  # Get previous month's sales\n",
    "\n",
    "# Apply Window Functions\n",
    "df_result = (\n",
    "    df.withColumn(\"Rank\", dense_rank().over(dept_window))\n",
    "    .withColumn(\"RunningTotalSales\", sum(\"Sales\").over(running_total_window))\n",
    "    .withColumn(\"DeptAvgSales\", avg(\"Sales\").over(dept_avg_window))\n",
    "    .withColumn(\"PreviousMonthSales\", lag(\"Sales\", 1).over(previous_sales_window))\n",
    ")\n",
    "\n",
    "# Show Result\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Many Functions Can We Apply in a Window Function?**\n",
    "\n",
    "In PySpark and SQL, **window functions** allow us to perform calculations across a set of table rows that are related to the current row. These functions can be classified into **three main categories**:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Ranking Functions**\n",
    "\n",
    "These functions assign a rank to each row within a partition.\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `RANK()` | Assigns a unique rank to rows. **Skips** numbers after ties. | 1, 2, 2, **4** |\n",
    "| `DENSE_RANK()` | Similar to `RANK()`, but **without gaps**. | 1, 2, 2, **3** |\n",
    "| `ROW_NUMBER()` | Assigns a unique number to each row. No ties. | 1, 2, 3, 4 |\n",
    "| `NTILE(n)` | Divides rows into **n** equal buckets. | Groups data into percentiles |\n",
    "\n",
    "✅ **PySpark Example**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number, ntile\n",
    "\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(\"Sales\")\n",
    "\n",
    "df.withColumn(\"Rank\", rank().over(window_spec)) \\\n",
    "  .withColumn(\"DenseRank\", dense_rank().over(window_spec)) \\\n",
    "  .withColumn(\"RowNum\", row_number().over(window_spec)) \\\n",
    "  .withColumn(\"Quartile\", ntile(4).over(window_spec)) \\\n",
    "  .show()\n",
    "```\n",
    "\n",
    "✅ **SQL Example**\n",
    "\n",
    "```sql\n",
    "SELECT Employee, Department, Sales,\n",
    "       RANK() OVER (PARTITION BY Department ORDER BY Sales) AS Rank,\n",
    "       DENSE_RANK() OVER (PARTITION BY Department ORDER BY Sales) AS DenseRank,\n",
    "       ROW_NUMBER() OVER (PARTITION BY Department ORDER BY Sales) AS RowNum,\n",
    "       NTILE(4) OVER (PARTITION BY Department ORDER BY Sales) AS Quartile\n",
    "FROM employee_sales;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Aggregate Functions**\n",
    "\n",
    "These functions perform calculations over a set of rows.\n",
    "\n",
    "| Function  | Description                   |\n",
    "| --------- | ----------------------------- |\n",
    "| `SUM()`   | Cumulative or running total.  |\n",
    "| `AVG()`   | Average of a partition.       |\n",
    "| `MIN()`   | Minimum value in a partition. |\n",
    "| `MAX()`   | Maximum value in a partition. |\n",
    "| `COUNT()` | Counts the number of rows.    |\n",
    "\n",
    "✅ **PySpark Example**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "df.withColumn(\"RunningTotal\", sum(\"Sales\").over(window_spec)) \\\n",
    "  .withColumn(\"AvgSales\", avg(\"Sales\").over(window_spec)) \\\n",
    "  .withColumn(\"MinSales\", min(\"Sales\").over(window_spec)) \\\n",
    "  .withColumn(\"MaxSales\", max(\"Sales\").over(window_spec)) \\\n",
    "  .withColumn(\"Count\", count(\"Sales\").over(window_spec)) \\\n",
    "  .show()\n",
    "```\n",
    "\n",
    "✅ **SQL Example**\n",
    "\n",
    "```sql\n",
    "SELECT Employee, Department, Sales,\n",
    "       SUM(Sales) OVER (PARTITION BY Department ORDER BY Sales ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS RunningTotal,\n",
    "       AVG(Sales) OVER (PARTITION BY Department) AS AvgSales,\n",
    "       MIN(Sales) OVER (PARTITION BY Department) AS MinSales,\n",
    "       MAX(Sales) OVER (PARTITION BY Department) AS MaxSales,\n",
    "       COUNT(Sales) OVER (PARTITION BY Department) AS Count\n",
    "FROM employee_sales;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Offset (Lag/Lead) and First/Last Functions**\n",
    "\n",
    "These functions help **compare** rows with previous or next ones.\n",
    "\n",
    "| Function        | Description                                 |\n",
    "| --------------- | ------------------------------------------- |\n",
    "| `LAG()`         | Retrieves the previous row's value.         |\n",
    "| `LEAD()`        | Retrieves the next row's value.             |\n",
    "| `FIRST_VALUE()` | Gets the first value in a partition.        |\n",
    "| `LAST_VALUE()`  | Gets the last value in a partition.         |\n",
    "| `NTH_VALUE(n)`  | Retrieves the **nth** value in a partition. |\n",
    "\n",
    "✅ **PySpark Example**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lag, lead, first, last\n",
    "\n",
    "df.withColumn(\"PrevMonthSales\", lag(\"Sales\", 1).over(window_spec)) \\\n",
    "  .withColumn(\"NextMonthSales\", lead(\"Sales\", 1).over(window_spec)) \\\n",
    "  .withColumn(\"FirstSale\", first(\"Sales\").over(window_spec)) \\\n",
    "  .withColumn(\"LastSale\", last(\"Sales\").over(window_spec)) \\\n",
    "  .show()\n",
    "```\n",
    "\n",
    "✅ **SQL Example**\n",
    "\n",
    "```sql\n",
    "SELECT Employee, Department, Sales,\n",
    "       LAG(Sales, 1) OVER (PARTITION BY Employee ORDER BY Month) AS PrevMonthSales,\n",
    "       LEAD(Sales, 1) OVER (PARTITION BY Employee ORDER BY Month) AS NextMonthSales,\n",
    "       FIRST_VALUE(Sales) OVER (PARTITION BY Department ORDER BY Sales) AS FirstSale,\n",
    "       LAST_VALUE(Sales) OVER (PARTITION BY Department ORDER BY Sales) AS LastSale\n",
    "FROM employee_sales;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🚀 Summary Table: All Window Functions**\n",
    "\n",
    "| Category        | Function        | Description                     |\n",
    "| --------------- | --------------- | ------------------------------- |\n",
    "| **Ranking**     | `RANK()`        | Skips numbers on ties           |\n",
    "|                 | `DENSE_RANK()`  | No rank gaps                    |\n",
    "|                 | `ROW_NUMBER()`  | Unique row number per partition |\n",
    "|                 | `NTILE(n)`      | Splits data into n buckets      |\n",
    "| **Aggregation** | `SUM()`         | Running total                   |\n",
    "|                 | `AVG()`         | Average of partition            |\n",
    "|                 | `MIN()`         | Minimum value                   |\n",
    "|                 | `MAX()`         | Maximum value                   |\n",
    "|                 | `COUNT()`       | Count rows in partition         |\n",
    "| **Offset**      | `LAG(n)`        | Get previous row's value        |\n",
    "|                 | `LEAD(n)`       | Get next row's value            |\n",
    "|                 | `FIRST_VALUE()` | First value in partition        |\n",
    "|                 | `LAST_VALUE()`  | Last value in partition         |\n",
    "|                 | `NTH_VALUE(n)`  | Nth value in partition          |\n",
    "\n",
    "---\n",
    "\n",
    "### **How Many Functions Can We Apply at Once?**\n",
    "\n",
    "There is **no strict limit** on the number of window functions you can apply **simultaneously**, but:\n",
    "\n",
    "- **Performance Consideration**: Applying too many window functions can slow down execution.\n",
    "- **Memory Constraints**: Large datasets with multiple window functions may require **high memory allocation**.\n",
    "\n",
    "💡 **Best Practice:** Apply only the necessary functions to optimize query performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **🔥 Advanced Challenge**\n",
    "\n",
    "Would you like me to generate an **extremely complex** example combining all these window functions in PySpark and SQL? 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
