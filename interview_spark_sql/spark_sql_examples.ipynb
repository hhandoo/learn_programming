{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8618d53",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5ab22c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window, WindowSpec\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType, BooleanType, StructType, StructField, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, min, max, lead, lag, rank, dense_rank, expr, minute, unix_timestamp, month, day, when, sum, date_trunc, date_diff, count, concat, lit, desc, asc\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c4d0e",
   "metadata": {},
   "source": [
    "# My Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "70eedad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572d60d",
   "metadata": {},
   "source": [
    "# Generate Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "10f6f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 50\n",
    "data = [\n",
    "    (\n",
    "        i + 1,                      # id\n",
    "        fake.first_name(),          # first_name\n",
    "        fake.last_name(),           # last_name\n",
    "        random.randint(18, 80),     # age\n",
    "        fake.email(),               # email\n",
    "        fake.city()                 # city\n",
    "    )\n",
    "    for i in range(num_records)\n",
    "]\n",
    "columns = [\"id\", \"first_name\", \"last_name\", \"age\", \"email\", \"city\"]\n",
    "\n",
    "test_data_frame = spark.createDataFrame(data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7b44166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+--------------------+-------------------+\n",
      "| id|first_name|last_name|age|               email|               city|\n",
      "+---+----------+---------+---+--------------------+-------------------+\n",
      "|  1|    Thomas|Hernandez| 61|jamesmiller@examp...|Port Katherinemouth|\n",
      "|  2|   Sabrina| Trujillo| 64|ccunningham@examp...|         East David|\n",
      "|  3|     Jason|    Olson| 60|wrightbrianna@exa...|    Mitchellchester|\n",
      "|  4|      Eric|      Cox| 37|    dlee@example.com|      Lake Kyletown|\n",
      "|  5|    Pamela|    Joyce| 77|cindyburke@exampl...|       Andrechester|\n",
      "|  6|      Ryan|   Wright| 31|denise08@example.net|        Desireebury|\n",
      "|  7|    Joseph|  Gilbert| 69|jennifer34@exampl...|         Kellyhaven|\n",
      "|  8|    Cassie|Rodriguez| 65|courtney66@exampl...|       Allisonmouth|\n",
      "|  9|     Megan|   Flores| 33|adamsdaniel@examp...|          Olsenland|\n",
      "| 10|     Amber|    Ortiz| 79|christinekelly@ex...|         Davisshire|\n",
      "| 11| Alejandra|  Allison| 45|shelia14@example.com|         Jamesmouth|\n",
      "| 12|       Amy| Campbell| 56|boylerhonda@examp...|      Garciachester|\n",
      "| 13|    Victor|     Hill| 78|cristina46@exampl...|         Deniseland|\n",
      "| 14|    Samuel|    Perez| 66|whitneybrown@exam...|         South Lisa|\n",
      "| 15|   Jessica|    Nunez| 53|  kgreen@example.org|        New Krystal|\n",
      "| 16|      Lisa|  Roberts| 43|steven37@example.com|         New Denise|\n",
      "| 17|   Charles|  Maxwell| 45|kristenlloyd@exam...|    Lake Davidville|\n",
      "| 18|    Ashley|    Green| 42|rhondastafford@ex...|    Port Rachelfort|\n",
      "| 19| Christina|   Bowers| 57|debraharris@examp...|   Port Danielhaven|\n",
      "| 20|     Shawn| Williams| 57|banksjennifer@exa...|          Craigtown|\n",
      "+---+----------+---------+---+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7190a",
   "metadata": {},
   "source": [
    "# Repeated Payments [Stripe SQL Interview Question]\n",
    "Stripe asked this tricky SQL interview question, about identifying any payments made at the same merchant with the same credit card for the same amount within 10 minutes of each other and reporting the count of such repeated payments.\n",
    "\n",
    "- same merchant\n",
    "- same credit card\n",
    "- same amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "08df4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"merchant_id\", IntegerType(), True),\n",
    "    StructField(\"credit_card_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"transaction_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 101, 1, 100, datetime.strptime('09/25/2022 12:00:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (2, 101, 1, 100, datetime.strptime('09/25/2022 12:08:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (3, 101, 1, 100, datetime.strptime('09/25/2022 12:28:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (4, 102, 2, 300, datetime.strptime('09/25/2022 12:00:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (6, 102, 2, 400, datetime.strptime('09/25/2022 14:00:00', '%m/%d/%Y %H:%M:%S'))\n",
    "]\n",
    "question_1 = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1c426a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_spec = Window.partitionBy([\"merchant_id\", \"credit_card_id\", \"amount\"]).orderBy(\"transaction_timestamp\")\n",
    "answer_1 = question_1\\\n",
    ".withColumn('prev_time', lag(\"transaction_timestamp\").over(window_spec))\\\n",
    ".withColumn('time_diff',  (unix_timestamp(col('transaction_timestamp')) - unix_timestamp(col('prev_time'))) / 60)\\\n",
    ".filter(\"time_diff<=10\")\n",
    "answer_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20f144",
   "metadata": {},
   "source": [
    "## SQL Query\n",
    "\n",
    "```sql\n",
    "\n",
    "WITH\n",
    "  payments AS (\n",
    "    SELECT\n",
    "      *,\n",
    "    EXTRACT(EPOCH FROM \n",
    "\n",
    "            transaction_timestamp - LAG(transaction_timestamp) \n",
    "            \n",
    "            OVER (PARTITION BY merchant_id, credit_card_id, amount ORDER BY transaction_timestamp)\n",
    "            \n",
    "            )/60\n",
    "            AS minute_difference\n",
    "    FROM\n",
    "      transactions\n",
    "  )\n",
    "SELECT\n",
    "  COUNT(merchant_id) AS payment_count\n",
    "FROM\n",
    "  payments\n",
    "WHERE\n",
    "  minute_difference <= 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598721b1",
   "metadata": {},
   "source": [
    "## Alternate solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cc91ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------+------+---------------------+--------------+-----------+--------------+------+---------------------+\n",
      "|transaction_id|merchant_id|credit_card_id|amount|transaction_timestamp|transaction_id|merchant_id|credit_card_id|amount|transaction_timestamp|\n",
      "+--------------+-----------+--------------+------+---------------------+--------------+-----------+--------------+------+---------------------+\n",
      "|             1|        101|             1|   100|  2022-09-25 12:00:00|             2|        101|             1|   100|  2022-09-25 12:08:00|\n",
      "+--------------+-----------+--------------+------+---------------------+--------------+-----------+--------------+------+---------------------+\n",
      "\n",
      "Repeated payments within 10 minutes: 1\n"
     ]
    }
   ],
   "source": [
    "# Self-join the DataFrame with itself\n",
    "df_alias1 = question_1.alias(\"t1\")\n",
    "df_alias2 = question_1.alias(\"t2\")\n",
    "\n",
    "# Join with conditions\n",
    "joined = df_alias1.join(\n",
    "    df_alias2,\n",
    "    (col(\"t1.merchant_id\") == col(\"t2.merchant_id\")) &\n",
    "    (col(\"t1.credit_card_id\") == col(\"t2.credit_card_id\")) &\n",
    "    (col(\"t1.amount\") == col(\"t2.amount\")) &\n",
    "\n",
    "    (col(\"t2.transaction_timestamp\") > col(\"t1.transaction_timestamp\")) &\n",
    "    (col(\"t2.transaction_timestamp\") <= expr(\"t1.transaction_timestamp + interval 10 minutes\")),\n",
    "    \n",
    "    how = \"inner\"\n",
    ")\n",
    "\n",
    "# Count repeated payments\n",
    "repeated_count = joined.count()\n",
    "\n",
    "joined.show()\n",
    "\n",
    "print(\"Repeated payments within 10 minutes:\", repeated_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db900f9d",
   "metadata": {},
   "source": [
    "## SQL Query\n",
    "\n",
    "```sql\n",
    "select\n",
    "  count(*)\n",
    "from\n",
    "  transactions as t1\n",
    "  inner join transactions as t2 on t1.merchant_id = t2.merchant_id\n",
    "  AND t1.credit_card_id = t2.credit_card_id\n",
    "  AND t1.amount = t2.amount\n",
    "  AND t2.transaction_timestamp > t1.transaction_timestamp\n",
    "  AND t2.transaction_timestamp <= t1.transaction_timestamp + INTERVAL '10 minute';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74106a4d",
   "metadata": {},
   "source": [
    "# Recursive CTE Example\n",
    "\n",
    "```sql\n",
    "WITH RECURSIVE EmployeeHierarchy AS (\n",
    "    -- Anchor member: Select the top-level employee(s)\n",
    "    SELECT EmployeeID, FirstName, LastName, ManagerID, 0 AS Level\n",
    "    FROM Employees\n",
    "    WHERE ManagerID IS NULL -- Assuming the top manager has NULL ManagerID\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Recursive member: Join with the CTE itself to find subordinates\n",
    "    SELECT e.EmployeeID, e.FirstName, e.LastName, e.ManagerID, eh.Level + 1\n",
    "    FROM Employees e\n",
    "    INNER JOIN EmployeeHierarchy eh ON e.ManagerID = eh.EmployeeID\n",
    ")\n",
    "SELECT *\n",
    "FROM EmployeeHierarchy\n",
    "ORDER BY Level, FirstName;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbdfc8",
   "metadata": {},
   "source": [
    "# Median Google Search Frequency [Google SQL Interview Question]\n",
    "\n",
    "Google’s Marketing Team needed to add a simple statistic to their upcoming Superbowl Ad: the median number of searches made per year. You were given a summary table that tells you the number of searches made last year, write a query to report the median searches made per user.\n",
    "\n",
    "```sql\n",
    "WITH RECURSIVE expanded AS (\n",
    "    -- Initialize the recursion with the original data\n",
    "    SELECT searches, num_users, 1 AS user_index\n",
    "    FROM searches\n",
    "    WHERE num_users > 0\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Recursively add rows based on num_users\n",
    "    SELECT e.searches, e.num_users, e.user_index + 1\n",
    "    FROM expanded e\n",
    "    WHERE e.user_index + 1 <= e.num_users\n",
    "),\n",
    "ordered AS (\n",
    "  SELECT \n",
    "  searches, \n",
    "  ROW_NUMBER() OVER (ORDER BY searches) AS rn,\n",
    "  COUNT(*) OVER () AS total_users\n",
    "  FROM expanded\n",
    ")\n",
    "SELECT\n",
    "    round(AVG(searches)::NUMERIC, 2) AS median\n",
    "FROM ordered\n",
    "WHERE rn IN (total_users/2, total_users/2 + 2);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9064b",
   "metadata": {},
   "source": [
    "# Monthly Merchant Balance [Visa SQL Interview Question]\n",
    "Say you have access to all the transactions for a given merchant account. Write a query to print the cumulative balance of the merchant account at the end of each day, with the total balance reset back to zero at the end of the month. Output the transaction date and cumulative balance.\n",
    "\n",
    "\n",
    "\n",
    "```sql\n",
    "WITH daily_balances AS (\n",
    "  SELECT\n",
    "    DATE_TRUNC('day', transaction_date) AS transaction_day,\n",
    "    DATE_TRUNC('month', transaction_date) AS transaction_month,\n",
    "    SUM(CASE WHEN type = 'deposit' THEN amount\n",
    "      WHEN type = 'withdrawal' THEN -amount END) AS balance\n",
    "  FROM visatransactions\n",
    "  GROUP BY \n",
    "    DATE_TRUNC('day', transaction_date),\n",
    "    DATE_TRUNC('month', transaction_date)\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  transaction_day,\n",
    "  SUM(balance) OVER (\n",
    "    PARTITION BY transaction_month\n",
    "    ORDER BY transaction_day) AS balance\n",
    "FROM daily_balances\n",
    "ORDER BY transaction_day;\n",
    "```\n",
    "\n",
    "Hint\n",
    "\n",
    "    Row\tCumulative Sum\n",
    "    1\t106.66\n",
    "    2\t106.66 + 98.50 = 205.16\n",
    "    3\t106.66 + 98.50 + 50.00 = 255.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f7919777",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (19153, 'deposit', 65.90, datetime.strptime('07/10/2022 10:00:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (53151, 'deposit', 178.55, datetime.strptime('07/08/2022 10:00:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (29776, 'withdrawal', 25.90, datetime.strptime('07/08/2022 10:00:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (16461, 'withdrawal', 45.99, datetime.strptime('07/08/2022 10:00:00', '%m/%d/%Y %H:%M:%S')),\n",
    "    (77134, 'deposit', 32.60, datetime.strptime('07/10/2022 10:00:00', '%m/%d/%Y %H:%M:%S'))\n",
    "]\n",
    "\n",
    "\n",
    "question_3 = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7065fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+-------------------+\n",
      "|transaction_id|      type|amount|   transaction_date|\n",
      "+--------------+----------+------+-------------------+\n",
      "|         19153|   deposit|  65.9|2022-07-10 10:00:00|\n",
      "|         53151|   deposit|178.55|2022-07-08 10:00:00|\n",
      "|         29776|withdrawal|  25.9|2022-07-08 10:00:00|\n",
      "|         16461|withdrawal| 45.99|2022-07-08 10:00:00|\n",
      "|         77134|   deposit|  32.6|2022-07-10 10:00:00|\n",
      "+--------------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "481133cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+\n",
      "|              month|                day|balance|\n",
      "+-------------------+-------------------+-------+\n",
      "|2022-07-01 00:00:00|2022-07-10 00:00:00|   98.5|\n",
      "|2022-07-01 00:00:00|2022-07-08 00:00:00| 106.66|\n",
      "+-------------------+-------------------+-------+\n",
      "\n",
      "+-------------------+-------+\n",
      "|                day|balance|\n",
      "+-------------------+-------+\n",
      "|2022-07-08 00:00:00| 106.66|\n",
      "|2022-07-10 00:00:00| 205.16|\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_3_group1 = question_3\\\n",
    "    .withColumn('month', date_trunc(\"month\", col(\"transaction_date\")))\\\n",
    "    .withColumn('day', date_trunc(\"day\", col(\"transaction_date\")))\n",
    "\n",
    "question_3_group1 = question_3_group1.groupBy([\"month\", \"day\"]).agg(\n",
    "    sum(\n",
    "        when(\n",
    "            col(\"type\") == \"deposit\", \n",
    "            col(\"amount\")\n",
    "        ).otherwise(\n",
    "            -1*col(\"amount\")\n",
    "        )\n",
    "        ).alias(\"balance\")\n",
    ")\n",
    "question_3_window = Window.partitionBy(\"month\").orderBy(\"day\")\n",
    "question_3_group1.show()\n",
    "answer_3 = question_3_group1.withColumn(\n",
    "    \"balance\", sum(\"balance\").over(question_3_window)) \\\n",
    "    .select(\"day\", \"balance\") \\\n",
    "    .orderBy(\"day\")\n",
    "\n",
    "answer_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81217f",
   "metadata": {},
   "source": [
    "# Server Utilization Time [Amazon SQL Interview Question]\n",
    "\n",
    "Fleets of servers power Amazon Web Services (AWS). Senior management has requested data-driven solutions to optimize server usage.\n",
    "\n",
    "Write a query that calculates the total time that the fleet of servers was running. The output should be in units of full days.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- Each server might start and stop several times.\n",
    "- The total time in which the server fleet is running can be calculated as the sum of each server's uptime.\n",
    "\n",
    "\n",
    "``` sql\n",
    "with running_time as  (\n",
    "  select \n",
    "  server_id,\n",
    "  session_status,\n",
    "  status_time AS start_time,\n",
    "  LEAD(status_time) OVER(partition by server_id order by status_time) as stop_time\n",
    "  from server_utilization\n",
    ")\n",
    "SELECT \n",
    "DATE_PART('days', JUSTIFY_HOURS(SUM(stop_time - start_time))) AS total_uptime_days\n",
    "FROM running_time\n",
    "WHERE session_status = 'start' AND stop_time IS NOT NULL;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7d98c1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------+\n",
      "|server_id|        status_time|session_status|\n",
      "+---------+-------------------+--------------+\n",
      "|        1|2022-08-02 10:00:00|         start|\n",
      "|        1|2022-08-04 10:00:00|          stop|\n",
      "|        2|2022-08-17 10:00:00|         start|\n",
      "|        2|2022-08-24 10:00:00|          stop|\n",
      "+---------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"server_id\", IntegerType(), True),\n",
    "    StructField(\"status_time\", TimestampType(), True),\n",
    "    StructField(\"session_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the data\n",
    "data = [\n",
    "    (1, datetime.strptime(\"2022-08-02 10:00:00\", \"%Y-%m-%d %H:%M:%S\"), \"start\"),\n",
    "    (1, datetime.strptime(\"2022-08-04 10:00:00\", \"%Y-%m-%d %H:%M:%S\"), \"stop\"),\n",
    "    (2, datetime.strptime(\"2022-08-17 10:00:00\", \"%Y-%m-%d %H:%M:%S\"), \"start\"),\n",
    "    (2, datetime.strptime(\"2022-08-24 10:00:00\", \"%Y-%m-%d %H:%M:%S\"), \"stop\")\n",
    "]\n",
    "\n",
    "question_4 = spark.createDataFrame(data, schema)\n",
    "\n",
    "question_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d962de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|total_diff|\n",
      "+----------+\n",
      "|         9|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_4_window = Window.partitionBy(\"server_id\").orderBy(\"status_time\")\n",
    "\n",
    "question_4 = question_4\\\n",
    ".withColumn('stop_time', lead(\"status_time\").over(question_4_window))\\\n",
    ".withColumn('diff', date_diff(col(\"stop_time\"),col(\"status_time\")))\\\n",
    ".filter(\"session_status = 'start' AND stop_time is not null\")\n",
    "\n",
    "\n",
    "answer_4 = question_4.agg(sum(col('diff')).alias('total_diff'))\n",
    "\n",
    "\n",
    "answer_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356db889",
   "metadata": {},
   "source": [
    "# Uniquely Staffed Consultants [Accenture SQL Interview Questions]\n",
    "\n",
    "As a Data Analyst on the People Operations team at Accenture, you are tasked with understanding how many consultants are staffed to each client, and how many consultants are exclusively staffed to a single client.\n",
    "\n",
    "Write a query that displays the outputs of client name and the number of uniquely and exclusively staffed consultants ordered by client name.\n",
    "\n",
    "```sql query\n",
    "with excusive_employees as (\n",
    "    select \n",
    "    e.employee_id\n",
    "    from \n",
    "    employee as e\n",
    "    left join\n",
    "    consulting_engagements as ce\n",
    "    on\n",
    "    e.engagement_id = ce.engagement_id\n",
    "        group by e.employee_id\n",
    "    having count(ce.client_name) = 1\n",
    ")\n",
    "select \n",
    "ce.client_name,\n",
    "count(e.employee_id) as total_staffed,\n",
    "count(ee.employee_id) as exclusive_staffed\n",
    "from \n",
    "employee as e\n",
    "left join\n",
    "consulting_engagements as ce\n",
    "on\n",
    "e.engagement_id = ce.engagement_id\n",
    "\n",
    "left join\n",
    "excusive_employees as ee\n",
    "on\n",
    "ee.employee_id = e.employee_id\n",
    "group by ce.client_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "38ae6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|engagement_id|\n",
      "+-----------+-------------+\n",
      "|       1001|            1|\n",
      "|       1001|            2|\n",
      "|       1002|            1|\n",
      "|       1003|            3|\n",
      "|       1004|            4|\n",
      "+-----------+-------------+\n",
      "\n",
      "+-------------+--------------------+--------------------+\n",
      "|engagement_id|        project_name|         client_name|\n",
      "+-------------+--------------------+--------------------+\n",
      "|            1|SAP Logistics Mod...|Department of Def...|\n",
      "|            2|Oracle Cloud Migr...|Department of Edu...|\n",
      "|            3|Trust & Safety Op...|              Google|\n",
      "|            4|SAP IoT Cloud Int...|              Google|\n",
      "+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_engagement_data = [\n",
    "    (1001, 1),\n",
    "    (1001, 2),\n",
    "    (1002, 1),\n",
    "    (1003, 3),\n",
    "    (1004, 4)\n",
    "]\n",
    "\n",
    "employee_engagement_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"engagement_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_engagement_df = spark.createDataFrame(employee_engagement_data, schema=employee_engagement_schema)\n",
    "\n",
    "consulting_engagement_data = [\n",
    "    (1, \"SAP Logistics Modernization\", \"Department of Defense\"),\n",
    "    (2, \"Oracle Cloud Migration\", \"Department of Education\"),\n",
    "    (3, \"Trust & Safety Operations\", \"Google\"),\n",
    "    (4, \"SAP IoT Cloud Integration\", \"Google\")\n",
    "]\n",
    "\n",
    "consulting_engagement_schema = StructType([\n",
    "    StructField(\"engagement_id\", IntegerType(), True),\n",
    "    StructField(\"project_name\", StringType(), True),\n",
    "    StructField(\"client_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "consulting_engagement_df = spark.createDataFrame(consulting_engagement_data, schema=consulting_engagement_schema)\n",
    "\n",
    "\n",
    "\n",
    "employee_engagement_df.show()\n",
    "consulting_engagement_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aff34ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------------+\n",
      "|         client_name|total_staffed|exclusive_staffed|\n",
      "+--------------------+-------------+-----------------+\n",
      "|Department of Def...|            2|                1|\n",
      "|Department of Edu...|            1|                0|\n",
      "|              Google|            2|                2|\n",
      "+--------------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_5_d1 = employee_engagement_df.alias('e').join(\n",
    "    consulting_engagement_df.alias('ce'),\n",
    "    (col('e.engagement_id') == col('ce.engagement_id')),\n",
    "    how = 'left'\n",
    ").select(\n",
    "    col('e.employee_id'),\n",
    "    col('ce.engagement_id'),\n",
    "    col('ce.project_name'),\n",
    "    col('ce.client_name')\n",
    ").groupBy('e.employee_id').agg(\n",
    "    count(\"ce.engagement_id\").alias(\"total_clients\")\n",
    ").filter('total_clients = 1')\n",
    "\n",
    "answer_5_d2 = employee_engagement_df.alias('e').join(\n",
    "    consulting_engagement_df.alias('ce'),\n",
    "    (col('e.engagement_id') == col('ce.engagement_id')),\n",
    "    how = 'left'\n",
    ").select(\n",
    "    col('e.employee_id'),\n",
    "    col('ce.engagement_id'),\n",
    "    col('ce.project_name'),\n",
    "    col('ce.client_name')\n",
    ")\n",
    "\n",
    "answer_5_d2 = answer_5_d2.alias('base_ds').join(\n",
    "    answer_5_d1.alias('ee'),\n",
    "    (col('base_ds.employee_id') == col('ee.employee_id')),\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "\n",
    "answer_5_d2 = answer_5_d2\\\n",
    "    .groupBy('base_ds.client_name')\\\n",
    "    .agg(\n",
    "        count('base_ds.employee_id').alias('total_staffed'),\n",
    "        count('ee.total_clients').alias('exclusive_staffed')\n",
    "    )\\\n",
    "    .orderBy('base_ds.client_name')\n",
    "\n",
    "\n",
    "answer_5_d2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b65ea8",
   "metadata": {},
   "source": [
    "# Event Friends Recommendation [Facebook SQL Interview Questions]\n",
    "\n",
    "Facebook wants to recommend new friends to people who show interest in attending 2 or more of the same private events.\n",
    "\n",
    "```sql\n",
    "WITH\n",
    "  private_events AS (\n",
    "    SELECT\n",
    "      user_id,\n",
    "      event_id\n",
    "    FROM\n",
    "      event_rsvp\n",
    "    WHERE\n",
    "      attendance_status IN ('going', 'maybe')\n",
    "      AND event_type = 'private'\n",
    "  )\n",
    "SELECT\n",
    "  friends.user_a_id,\n",
    "  friends.user_b_id\n",
    "FROM\n",
    "  private_events AS events_1\n",
    "  INNER JOIN private_events AS events_2 ON events_1.user_id != events_2.user_id\n",
    "  AND events_1.event_id = events_2.event_id\n",
    "  INNER JOIN friendship_status AS friends ON events_1.user_id = friends.user_a_id\n",
    "  AND events_2.user_id = friends.user_b_id\n",
    "WHERE \n",
    "  friends.status = 'not_friends'\n",
    "GROUP BY\n",
    "  friends.user_a_id,\n",
    "  friends.user_b_id\n",
    "HAVING\n",
    "  COUNT(*) >= 2;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9893890",
   "metadata": {},
   "source": [
    "# 3-Topping Pizzas [McKinsey SQL Interview Question]\n",
    "You’re a consultant for a major pizza chain that will be running a promotion where all 3-topping pizzas will be sold for a fixed price, and are trying to understand the costs involved.\n",
    "\n",
    "Given a list of pizza toppings, consider all the possible 3-topping pizzas, and print out the total cost of those 3 toppings. Sort the results with the highest total cost on the top followed by pizza toppings in ascending order.\n",
    "\n",
    "Break ties by listing the ingredients in alphabetical order, starting from the first ingredient, followed by the second and third.\n",
    "\n",
    "```sql\n",
    "select \n",
    "    CONCAT(t1.topping_name, ',', t2.topping_name, ',', t3.topping_name) AS pizza,\n",
    "    t1.ingredient_cost + t2.ingredient_cost + t3.ingredient_cost AS total_cost\n",
    "from\n",
    "    topping as t1\n",
    "inner join\n",
    "    topping as t2\n",
    "on\n",
    "    t1.topping_name < t2.topping_name\n",
    "inner join\n",
    "    topping as t3\n",
    "on\n",
    "    t2.topping_name < t3.topping_name\n",
    "ORDER BY total_cost DESC, pizza;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dc8966ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topping_data = [\n",
    "    (\"Pepperoni\", 0.50),\n",
    "    (\"Sausage\", 0.70),\n",
    "    (\"Chicken\", 0.55),\n",
    "    (\"Extra Cheese\", 0.40)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "topping_schema = StructType([\n",
    "    StructField(\"topping_name\", StringType(), True),\n",
    "    StructField(\"ingredient_cost\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "topping_df = spark.createDataFrame(topping_data, schema=topping_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_6 = topping_df.alias(\"t1\")\\\n",
    ".join(\n",
    "    topping_df.alias(\"t2\"),\n",
    "    (\n",
    "        col(\"t1.topping_name\") < col(\"t2.topping_name\")\n",
    "    ),\n",
    "    \"inner\"\n",
    ")\\\n",
    ".join(\n",
    "    topping_df.alias(\"t3\"),\n",
    "    (\n",
    "        col(\"t2.topping_name\") < col(\"t3.topping_name\")\n",
    "    ),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "answer_6 = answer_6.withColumn(\"pizza\",\n",
    "    concat(\n",
    "        col(\"t1.topping_name\"), \n",
    "        lit(\",\"),\n",
    "        col(\"t2.topping_name\"), \n",
    "        lit(\",\"),\n",
    "        col(\"t3.topping_name\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "answer_6 = answer_6.withColumn(\"total_cost\",col(\"t1.ingredient_cost\")+col(\"t2.ingredient_cost\")+col(\"t3.ingredient_cost\")  )\n",
    "\n",
    "\n",
    "\n",
    "answer_6.select(\"pizza\", \"total_cost\").orderBy(desc(\"total_cost\"), asc(\"pizza\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5eded5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 751:====================================================>(503 + 8) / 512]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------+\n",
      "|pizza                         |total_cost        |\n",
      "+------------------------------+------------------+\n",
      "|Chicken,Pepperoni,Sausage     |1.75              |\n",
      "|Chicken,Extra Cheese,Sausage  |1.65              |\n",
      "|Extra Cheese,Pepperoni,Sausage|1.6               |\n",
      "|Chicken,Extra Cheese,Pepperoni|1.4500000000000002|\n",
      "+------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "answer_6.select(\"pizza\", \"total_cost\").orderBy(desc(\"total_cost\"), asc(\"pizza\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852d1be",
   "metadata": {},
   "source": [
    "# Follow-Up Airpod Percentage [Apple SQL Interview Questions]\n",
    "The Apple retention team needs your help to investigate buying patterns. Write a query to determine the percentage of buyers who bought AirPods directly after they bought iPhones. Round your answer to a percentage (i.e. 20 for 20%, 50 for 50) with no decimals.\n",
    "\n",
    "\n",
    "```sql\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
