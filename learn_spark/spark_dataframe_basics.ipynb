{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    desc,\n",
    "    asc,\n",
    "    count_distinct,\n",
    "    avg,\n",
    "    stddev,\n",
    "    format_number,\n",
    "    mean,\n",
    "    dayofmonth,\n",
    "    dayofyear,\n",
    "    month,\n",
    "    year,\n",
    "    hour,\n",
    "    weekofyear,\n",
    "    date_format,\n",
    "    spark_partition_id,\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_per_partition(input_df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        input_df.withColumn(\"spark_partition_id\", spark_partition_id())\n",
    "        .groupBy(\"spark_partition_id\")\n",
    "        .count()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark Session\n",
    "\n",
    "> Builder is a class whereas builder initializes the Builder class\n",
    "\n",
    "## Understanding The Spark Session\n",
    "\n",
    "A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern.\n",
    "\n",
    "## Methods\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `active()` | Returns the active or default SparkSession for the current thread, returned by the builder. |\n",
    "| `addArtifact(*path[, pyfile, archive, file])` | Add artifact(s) to the client session. |\n",
    "| `addArtifacts(*path[, pyfile, archive, file])` | Add artifact(s) to the client session. |\n",
    "| `addTag(tag)` | Add a tag to be assigned to all the operations started by this thread in this session. |\n",
    "| `clearTags()` | Clear the current thread’s operation tags. |\n",
    "| `copyFromLocalToFs(local_path, dest_path)` | Copy file from local to cloud storage file system. |\n",
    "| `createDataFrame(data[, schema, …])` | Creates a DataFrame from an RDD, a list, a pandas.DataFrame, or a numpy.ndarray. |\n",
    "| `getActiveSession()` | Returns the active SparkSession for the current thread, returned by the builder. |\n",
    "| `getTags()` | Get the tags that are currently set to be assigned to all the operations started by this thread. |\n",
    "| `interruptAll()` | Interrupt all operations of this session currently running on the connected server. |\n",
    "| `interruptOperation(op_id)` | Interrupt an operation of this session with the given operationId. |\n",
    "| `interruptTag(tag)` | Interrupt all operations of this session with the given operation tag. |\n",
    "| `newSession()` | Returns a new SparkSession as a new session, that has separate SQLConf, registered temporary views, and UDFs, but shared SparkContext and table cache. |\n",
    "| `range(start[, end, step, numPartitions])` | Create a DataFrame with a single `pyspark.sql.types.LongType` column named `id`, containing elements in a range from start to end (exclusive) with step value `step`. |\n",
    "| `removeTag(tag)` | Remove a tag previously added to be assigned to all the operations started by this thread in this session. |\n",
    "| `sql(sqlQuery[, args])` | Returns a DataFrame representing the result of the given query. |\n",
    "| `stop()` | Stop the underlying SparkContext. |\n",
    "| `table(tableName)` | Returns the specified table as a DataFrame. |\n",
    "\n",
    "## Attributes\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| `builder` | Provides access to the Builder class for creating SparkSession. |\n",
    "| `catalog` | Interface for creating, dropping, altering, or querying underlying databases, tables, functions, etc. |\n",
    "| `client` | Gives access to the Spark Connect client. |\n",
    "| `conf` | Runtime configuration interface for Spark. |\n",
    "| `read` | Returns a `DataFrameReader` that can be used to read data as a DataFrame. |\n",
    "| `readStream` | Returns a `DataStreamReader` that can be used to read data streams as a streaming DataFrame. |\n",
    "| `sparkContext` | Returns the underlying `SparkContext`. |\n",
    "| `streams` | Returns a `StreamingQueryManager` that allows managing all the `StreamingQuery` instances active on this context. |\n",
    "| `udf` | Returns a `UDFRegistration` for UDF registration. |\n",
    "| `udtf` | Returns a `UDTFRegistration` for UDTF registration. |\n",
    "| `version` | The version of Spark on which this application is running. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/20 23:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/20 23:19:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/20 23:19:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/20 23:19:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- active() : Returns the active or default SparkSession for the current thread, returned by the builder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd049de2270>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.active()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- getActiveSession() : Returns the active SparkSession for the current thread, returned by the builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd049de2270>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- newSession() : Returns a new SparkSession as new session, that has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fd0580d6520>\n"
     ]
    }
   ],
   "source": [
    "spark = spark.newSession()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- range(start[, end, step, numPartitions]) : Create a DataFrame with single pyspark.sql.types.LongType column named id, containing elements in a range from start to end (exclusive) with step value step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1, 1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Levels in Spark\n",
    "\n",
    "## Memory only Storage level\n",
    "\n",
    "StorageLevel.MEMORY_ONLY is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is not enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required.\n",
    "\n",
    "This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions, and recomputing the in-memory columnar representation of the underlying table is expensive.\n",
    "\n",
    "## Serialize in Memory\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_SER is the same as MEMORY_ONLY but the difference being it stores `RDD as serialized objects to JVM memory.` It takes lesser memory (space-efficient) than MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "## Memory only and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_2 is same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Serialized in Memory and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_SER_2 is same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Memory and Disk Storage level\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as deserialized objects. When required storage is greater than available memory, it stores some of the excess partitions into a disk and reads the data from the disk when required. It is slower as there is I/O involved.\n",
    "\n",
    "## Serialize in Memory and Disk\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_SER is same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.\n",
    "\n",
    "## Memory, Disk and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_2 is Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Serialize in Memory, Disk and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_SER_2 is same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Disk only storage level\n",
    "\n",
    "In StorageLevel.DISK_ONLY storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O involved.\n",
    "\n",
    "## Disk only and Replicate\n",
    "\n",
    "StorageLevel.DISK_ONLY_2 is same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "| Storage Level       | Space used | CPU time | In memory | On-disk | Serialized | Recompute some partitions |\n",
    "| ------------------- | ---------- | -------- | --------- | ------- | ---------- | ------------------------- |\n",
    "| MEMORY_ONLY         | High       | Low      | Y         | N       | N          | Y                         |\n",
    "| MEMORY_ONLY_SER     | Low        | High     | Y         | N       | Y          | Y                         |\n",
    "| MEMORY_AND_DISK     | High       | Medium   | Some      | Some    | Some       | N                         |\n",
    "| MEMORY_AND_DISK_SER | Low        | High     | Some      | Some    | Y          | N                         |\n",
    "| DISK_ONLY           | Low        | High     | N         | Y       | Y          | N                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Dataset\n",
    "\n",
    "## Spark DataFrameReader\n",
    "\n",
    "Use `SparkSession.read` to access this.\n",
    "\n",
    "Quick function review:\n",
    "\n",
    "- `csv(path)`\n",
    "- `jdbc(url, table, ..., connectionProperties)`\n",
    "- `json(path)`\n",
    "- `format(source)`\n",
    "- `load(path)`\n",
    "- `orc(path)`\n",
    "- `parquet(path)`\n",
    "- `table(tableName)`\n",
    "- `text(path)`\n",
    "- `textFile(path)`\n",
    "\n",
    "Configuration methods:\n",
    "\n",
    "- `option(key, value)`\n",
    "- `options(map)`\n",
    "- `schema(schema)`\n",
    "\n",
    "### Header:\n",
    "\n",
    "If the csv file have a header (column names in the first row) then set header=true. This will use the first row in the csv file as the dataframe's column names. Setting header=false (default option) will result in a dataframe with default column names: \\_c0, \\_c1, \\_c2, etc.\n",
    "\n",
    "Setting this to true or false should be based on your input file.\n",
    "\n",
    "### Schema:\n",
    "\n",
    "The schema refered to here are the column types. A column can be of type String, Double, Long, etc. Using inferSchema=false (default option) will give a dataframe where all columns are strings (StringType). Depending on what you want to do, strings may not work. For example, if you want to add numbers from different columns, then those columns should be of some numeric type (strings won't work).\n",
    "\n",
    "By setting inferSchema=true, Spark will automatically go through the csv file and infer the schema of each column. This requires an extra pass over the file which will result in reading a file with inferSchema set to true being slower. But in return the dataframe will most likely have a correct schema given its input.\n",
    "\n",
    "### Faster Method\n",
    "\n",
    "As an alternative to reading a csv with inferSchema you can provide the schema while reading. This have the advantage of being faster than inferring the schema while giving a dataframe with the correct column types. In addition, for csv files without a header row, column names can be given automatically. To provde schema see e.g.: Provide schema while reading csv file as a dataframe\n",
    "\n",
    "### CSV w/InferSchema\n",
    "\n",
    "- we still have three columns\n",
    "- all three columns are still **nullable**\n",
    "- all three columns have their proper names\n",
    "- two jobs were executed (not one as in the previous example)\n",
    "- our three columns now have distinct data types:\n",
    "  - **timestamp** == **timestamp**\n",
    "  - **site** == **string**\n",
    "  - **requests** == **integer**\n",
    "\n",
    "**Question:** Why were there two jobs?\n",
    "\n",
    "inferSchema option tells the reader to infer data types from the source file. This results in an additional pass over the file resulting in two Spark jobs being triggered.\n",
    "\n",
    "**Question:** How long did the last job take?\n",
    "\n",
    "Command took 31.69 seconds -- by kaeshur_pirate@proton.me at 18/11/2023, 3:15:56 pm on My Cluster\n",
    "\n",
    "**Question:** Why did it take so much longer?\n",
    "\n",
    "Bec spark needs to scan the whole file in infer schema\n",
    "\n",
    "### CSV w/ User-Defined Schema\n",
    "\n",
    "- We still have three columns\n",
    "- All three columns are **NOT** nullable because we declared them as such.\n",
    "- All three columns have their proper names\n",
    "- Zero jobs were executed\n",
    "- Our three columns now have distinct data types:\n",
    "  - **timestamp** == **string**\n",
    "  - **site** == **string**\n",
    "  - **requests** == **integer**\n",
    "\n",
    "**Question:** Why were there no jobs?\n",
    "\n",
    "When you define the schema, Spark doesn’t need to perform the costly scan of the entire dataset to infer data types and structures. No need to read the header (line #1) or infer the schema (entire file).\n",
    "That same information is now declared in the user-defined schema.\n",
    "\n",
    "**Question:** What is different about the data types of these columns compared to the previous exercise & why?\n",
    "\n",
    "The timestamp column is now of type string because we declared it as such.\n",
    "\n",
    "**Question:** Do I need to indicate that the file has a header?\n",
    "\n",
    "Yes, otherwise, line #1 will be treated as data and not as a header.\n",
    "\n",
    "**Question:** Do the declared column names need to match the columns in the header of the TSV file?\n",
    "No, and you can demonstrate that by renaming it in the schema to something like capturedAt.\n",
    "When reading a CSV file using Apache Spark and defining well-defined data types, the column definitions specified in your code should align with the actual columns present in the CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Schema\n",
    "\n",
    "Pros\n",
    "\n",
    "- Simplicity: It’s easy to use, especially for quick data exploration or when the schema is not known in advance.\n",
    "- Less code: You don’t need to manually specify the schema, which reduces the amount of code you need to write.\n",
    "\n",
    "Cons\n",
    "\n",
    "- Performance Overhead: Spark needs to scan the entire dataset to infer the schema, which can be computationally expensive, especially for large datasets.\n",
    "- Data Quality: Inference may lead to incorrect schema deductions if the data has missing or inconsistent values.\n",
    "- Type Inference: Inferencing may not always correctly identify the data types of columns, leading to potential data type mismatches.\n",
    "\n",
    "### Define Schema Explicitly\n",
    "\n",
    "Pros\n",
    "\n",
    "- Performance: Defining the schema explicitly can significantly improve performance because Spark doesn’t need to scan the entire dataset to infer the schema.\n",
    "- Data Quality: You have control over the schema definition, ensuring that it accurately represents your data. This is important for data integrity and consistency.\n",
    "\n",
    "Cons\n",
    "\n",
    "- More Code: You need to write additional code to define the schema, which can be more cumbersome, especially for complex datasets or when the schema evolves over time.\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "- In terms of performance, defining the schema explicitly is generally more efficient than inferring it. When you define the schema, Spark doesn’t need to perform the costly scan of the entire dataset to infer data types and structures.\n",
    "\n",
    "- Explicit schema definition is particularly advantageous for large datasets where schema inference can introduce a significant overhead.\n",
    "\n",
    "- If data quality and performance are critical for your application, defining the schema explicitly is often the preferred approach, especially for production-level code.\n",
    "\n",
    "However, it’s essential to strike a balance between performance and development speed. In some scenarios, such as quick data exploration or ad-hoc analysis, inferring the schema might be acceptable. Ultimately, the choice between inferring and defining the schema should consider factors like data quality, development effort, and the specific requirements of your Spark application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reaing JSON Lines\n",
    "\n",
    "JSON Lines is a text file format that stores JSON values one per line, with a line separator of '\\\\n' or '\\\\r\\\\n'.\n",
    "\n",
    "This format is referred to as **JSON Lines** or **newline-delimited JSON**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.json(\"./datasets/people.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV Files\n",
    "\n",
    "The formats CSV [RFC4180] (comma separated values) and TSV [IANA-TSV] (tab separated values) provide simple, easy to process formats for the transmission of tabular data. They are supported as input datat formats to many tools, particularly spreadsheets. This document describes their use for expressing SPARQL query results from SELECT queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_stock_df = spark.read.options(inferSchema=True, header=True).csv(\n",
    "    \"./datasets/appl_stock.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_info_df = (\n",
    "    spark.read.option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"./datasets/sales_info.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_null_df = (\n",
    "    spark.read.option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"./datasets/ContainsNull.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files with custom Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "    StructField(name=\"age\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "]\n",
    "final_structure = StructType(fields=data_schema)\n",
    "\n",
    "people_df_custom_schema = spark.read.json(\n",
    "    \"./datasets/people.json\", schema=final_structure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data in Parquet Format\n",
    "\n",
    "Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. Parquet is available in multiple languages including Java, C++, Python, etc...\n",
    "\n",
    "#### About Parquet Files\n",
    "\n",
    "- Free & Open Source.\n",
    "- Increased query performance over row-based data stores.\n",
    "- Provides efficient data compression.\n",
    "- Designed for performance on large data sets.\n",
    "- Supports limited schema evolution.\n",
    "- Is a splittable \"file format\".\n",
    "- A <a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Column-Oriented</a> data store\n",
    "\n",
    "- We do not need to specify the schema - the column names and data types are stored in the parquet files.\n",
    "- Only one job is required to **read** that schema from the parquet file's metadata.\n",
    "- Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata.\n",
    "\n",
    "#### Read in the Parquet Files\n",
    "\n",
    "To read in this files, we will specify the location of the parquet directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.read.parquet(\"./datasets/flights.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.show() → None[source]\n",
    "\n",
    "Parameters\n",
    "\n",
    "1. n int, optional Number of rows to show.\n",
    "\n",
    "2. truncate bool or int, optional If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n",
    "3. vertical bool, optional If set to True, print output rows vertically (one line per column value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " age    | NULL    \n",
      " height | NULL    \n",
      " name   | NULL    \n",
      "-RECORD 1---------\n",
      " age    | NULL    \n",
      " height | NULL    \n",
      " name   | Michael \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show(truncate=False, n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.printSchema() → None\n",
    "\n",
    "1. level int, optional, default -> None : How many levels to print for nested schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_df = spark.createDataFrame([(1, (2, 2))], [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      " |    |-- _1: long (nullable = true)\n",
      " |    |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df.printSchema(1)\n",
    "nested_df.printSchema(2)\n",
    "del nested_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.describe() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "cols str, list, optional Column name or list of column names to describe by (default All columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-------+\n",
      "|summary|              age|height|   name|\n",
      "+-------+-----------------+------+-------+\n",
      "|  count|                4|     6|      7|\n",
      "|   mean|             24.5|  80.0|   NULL|\n",
      "| stddev|6.952217871538069|   0.0|   NULL|\n",
      "|    min|               18|    80|   Andy|\n",
      "|    max|               31|    80|Michael|\n",
      "+-------+-----------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|             24.5|\n",
      "| stddev|6.952217871538069|\n",
      "|    min|               18|\n",
      "|    max|               31|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.describe([\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.agg() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg()).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    exprs Column or dict of key and value strings, Columns or expressions to aggregate DataFrame by.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Aggregated DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      31|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.agg({\"age\": \"max\"}).show()\n",
    "# people_df.groupBy().agg({\"age\": \"max\"}).show() # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.alias(alias: str) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    alias str: an alias name to be set for the DataFrame.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Aliased DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+------+-------+\n",
      "| age|height|   name| age|height|   name|\n",
      "+----+------+-------+----+------+-------+\n",
      "|NULL|  NULL|Michael|NULL|    80|Michael|\n",
      "|NULL|  NULL|Michael|NULL|    80|Michael|\n",
      "|NULL|  NULL|Michael|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|  NULL|Michael|\n",
      "|  30|    80|   Andy|  31|    80|   Andy|\n",
      "|  30|    80|   Andy|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|  31|    80|   Andy|\n",
      "|  31|    80|   Andy|  30|    80|   Andy|\n",
      "|  18|    80| Justin|  19|    80| Justin|\n",
      "|  18|    80| Justin|  18|    80| Justin|\n",
      "|  19|    80| Justin|  19|    80| Justin|\n",
      "|  19|    80| Justin|  18|    80| Justin|\n",
      "+----+------+-------+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df_alias_1 = people_df.alias(\"people_df_1\")\n",
    "people_df_alias_2 = people_df.alias(\"people_df_2\")\n",
    "\n",
    "people_df_alias_1.join(\n",
    "    people_df_alias_2, col(\"people_df_1.name\") == col(\"people_df_2.name\"), how=\"inner\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.approxQuantile() → Union[List[float], List[List[float]]]\n",
    "\n",
    "Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p \\* N). More precisely,\n",
    "\n",
    "floor((p - err) _ N) <= rank(x) <= ceil((p + err) _ N).\n",
    "\n",
    "This method implements a variation of the Greenwald-Khanna algorithm (with some speed optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670 Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    1. col: str, tuple or list: Can be a single column name, or a list of names for multiple columns.\n",
    "\n",
    "    2. probabilities list or tuple: a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
    "\n",
    "    3. relativeError float: The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but gives the same result as 1.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    list\n",
    "    the approximate quantiles at the given probabilities.\n",
    "\n",
    "    If the input col is a string, the output is a list of floats.\n",
    "\n",
    "    If the input col is a list or tuple of strings, the output is also a\n",
    "    list, but each element in it is a list of floats, i.e., the output is a list of list of floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.0]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.approxQuantile(col=\"age\", probabilities=[0.5], relativeError=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.cache() → pyspark.sql.dataframe.DataFrame[source]\n",
    "\n",
    "Persists the DataFrame with the default storage level (MEMORY_AND_DISK).\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Cached DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, height: bigint, name: string]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.persist() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    storageLevel StorageLevel: Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Persisted DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, height: bigint, name: string]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.unpersist()) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Marks the DataFrame as non-persistent, and remove all blocks for it from memory and disk.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    blocking bool\n",
    "    Whether to block until all blocks are deleted.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Unpersisted DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, height: bigint, name: string]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.unpersist(blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.checkpoint() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a checkpointed version of this DataFrame. Checkpointing can be used to truncate the logical plan of this DataFrame, which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with SparkContext.setCheckpointDir().\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    eager bool, optional, default True: Whether to checkpoint this DataFrame immediately.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Checkpointed DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.coalesce(numPartitions: int) → pyspark.sql.dataframe.DataFrame[source]\n",
    "\n",
    "Similar to coalesce defined on an RDD, this operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can call repartition(). This will add a shuffle step, but means the current upstream partitions will be executed in parallel (per whatever the current partitioning is).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    numPartitions int\n",
    "    specify the target number of partitions\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.colRegex(colName: str) → pyspark.sql.column.Column\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    colName str\n",
    "    string, column name specified as a regex.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| age|height|\n",
      "+----+------+\n",
      "|NULL|  NULL|\n",
      "|NULL|  NULL|\n",
      "|NULL|    80|\n",
      "|NULL|    80|\n",
      "|  30|    80|\n",
      "|  31|    80|\n",
      "|  18|    80|\n",
      "|  19|    80|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(people_df.colRegex(\"`(name)?+.+`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.collect() → List[pyspark.sql.types.Row]\n",
    "\n",
    "Returns all the records as a list of Row.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    list\n",
    "    List of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, height=None, name=None),\n",
       " Row(age=None, height=None, name='Michael'),\n",
       " Row(age=None, height=80, name='Michael'),\n",
       " Row(age=None, height=80, name='Michael'),\n",
       " Row(age=30, height=80, name='Andy'),\n",
       " Row(age=31, height=80, name='Andy'),\n",
       " Row(age=18, height=80, name='Justin'),\n",
       " Row(age=19, height=80, name='Justin')]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.corr() → float\n",
    "\n",
    "Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    col1 str\n",
    "    The name of the first column\n",
    "\n",
    "    col2 str\n",
    "    The name of the second column\n",
    "\n",
    "    method str, optional\n",
    "    The correlation method. Currently only supports “pearson”\n",
    "\n",
    "### Returns\n",
    "\n",
    "    float\n",
    "    Pearson Correlation Coefficient of two columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.corr(\"age\", \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.createGlobalTempView(name: str) → None\n",
    "\n",
    "Creates a global temporary view with this DataFrame.\n",
    "The lifetime of this temporary view is tied to this `Spark application`. throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    name str\n",
    "    Name of the view.\n",
    "\n",
    "#### Register a Table/View\n",
    "\n",
    "- Databrick's UI has built in support for working with a number of different data sources\n",
    "- New ones are being added regularly\n",
    "- In our case we are going to upload the file <a href=\"http://files.training.databricks.com/static/data/pageviews_by_second_example.tsv\">pageviews_by_second_example.tsv</a>\n",
    "- .. and then use the UI to create a table.\n",
    "\n",
    "There are several benefits to this strategy:\n",
    "\n",
    "- Once setup, it never has to be done again\n",
    "- It is available for any user on the platform (permissions permitting)\n",
    "- Minimizes exposure of credentials\n",
    "- No real overhead to reading the schema (no infer-schema)\n",
    "- Easier to advertise available datasets to other users\n",
    "\n",
    "#### Review: Reading from Tables\n",
    "\n",
    "- No job is executed - the schema is stored in the table definition on Databricks.\n",
    "- The data types shown here are those we defined when we registered the table.\n",
    "- In our case, the file was uploaded to Databricks and is stored on the DBFS.\n",
    "  - If we used JDBC, it would open the connection to the database and read it in.\n",
    "  - If we used an object store (like what is backing the DBFS), it would read the data from source.\n",
    "- The \"registration\" of the table simply makes future access, or access by multiple users easier.\n",
    "- The users of the notebook cannot see username and passwords, secret keys, tokens, etc.\n",
    "\n",
    "** _Note #1:_ ** _The method createOrReplaceTempView(..) is bound to the SparkSession meaning it will be discarded once the session ends._\n",
    "\n",
    "** _Note #2:_ ** On the other hand, the method createOrReplaceGlobalTempView(..) is bound to the spark application.\\*\n",
    "\n",
    "`_Or to put that another way, I can use createOrReplaceTempView(..) in this notebook only. However, I can call createOrReplaceGlobalTempView(..) in this notebook and then access it from another._`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.createGlobalTempView(\"people_df_global_temp_view\")\n",
    "# spark.sql(\"select * from global_temp.people_df_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceGlobalTempView(\"people_df_global_temp_view\")\n",
    "spark.sql(\"select * from global_temp.people_df_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.createTempView(name: str) → None\n",
    "\n",
    "Creates a local temporary view with this DataFrame.\n",
    "\n",
    "The lifetime of this temporary table is tied to the `SparkSession` that was used to create this DataFrame.throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    name str\n",
    "    Name of the view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.createTempView(\"people_df_temp_view\")\n",
    "# spark.sql(\"select * from people_df_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceTempView(\"people_df_temp_view\")\n",
    "spark.sql(\"select * from people_df_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.crossJoin(other: pyspark.sql.dataframe.DataFrame) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns the cartesian product with another DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    other DataFrame\n",
    "    Right side of the cartesian product.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Joined DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+\n",
      "| age|height|   name| age|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|  30|\n",
      "|NULL|  NULL|   NULL|  31|\n",
      "|NULL|  NULL|   NULL|  18|\n",
      "|NULL|  NULL|   NULL|  19|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|  30|\n",
      "|NULL|  NULL|Michael|  31|\n",
      "|NULL|  NULL|Michael|  18|\n",
      "|NULL|  NULL|Michael|  19|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "+----+------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.crossJoin(people_df.select(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.distinct() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "#### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with distinct records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|  19|    80| Justin|\n",
      "|  30|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  31|    80|   Andy|\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.drop(\\*cols: ColumnOrName) → DataFrame[source]\n",
    "\n",
    "Returns a new DataFrame without specified columns. This is a no-op if the schema doesn’t contain the given column name(s).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols: str or :class:`Column`\n",
    "    a name of the column, or the Column to drop\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame without given columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|height|   name|\n",
      "+------+-------+\n",
      "|  NULL|   NULL|\n",
      "|  NULL|Michael|\n",
      "|    80|Michael|\n",
      "|    80|Michael|\n",
      "|    80|   Andy|\n",
      "|    80|   Andy|\n",
      "|    80| Justin|\n",
      "|    80| Justin|\n",
      "+------+-------+\n",
      "\n",
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.drop(\"age\").show()\n",
    "\n",
    "people_df.drop(\"agasdasdasde\").show()  ## no op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.dropDuplicates(subset: Optional[List[str]] = None) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Return a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n",
    "\n",
    "For a static batch DataFrame, it just drops duplicate rows. For a streaming DataFrame, it will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark() to limit how late the duplicate data can be and the system will accordingly limit the state. In addition, data older than watermark will be dropped to avoid any possibility of duplicates.\n",
    "\n",
    "`drop_duplicates() is an alias for dropDuplicates().`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    subset List of column names, optional\n",
    "    List of columns to use for duplicate comparison (default All columns).\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame without duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n",
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|  19|    80| Justin|\n",
      "|  30|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  31|    80|   Andy|\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "+----+------+-------+\n",
      "\n",
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|  30|    80|   Andy|\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|  18|    80| Justin|\n",
      "|NULL|    80|Michael|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "people_df.dropDuplicates().show()\n",
    "people_df.dropDuplicates(subset=[\"name\", \"height\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.dropna() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame omitting rows with null values. DataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    how str, optional\n",
    "    ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "    thresh: int, optional\n",
    "    default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "    subset str, tuple or list, optional\n",
    "    optional list of column names to consider.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with null only rows excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n",
      "+---+------+------+\n",
      "|age|height|  name|\n",
      "+---+------+------+\n",
      "| 30|    80|  Andy|\n",
      "| 31|    80|  Andy|\n",
      "| 18|    80|Justin|\n",
      "| 19|    80|Justin|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "# people_df.dropna(how=\"any\", subset=[\"height\"]).show()\n",
    "people_df.dropna(how=\"any\", subset=[\"height\", \"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.fillna() → DataFrame\n",
    "\n",
    "Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    value int, float, string, bool or dict\n",
    "    Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "    subset str, tuple or list, optional\n",
    "    optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "\n",
    "> The replacement datatype should be same as the column datatype\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with replaced null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n",
      "+---+------+-------+\n",
      "|age|height|   name|\n",
      "+---+------+-------+\n",
      "| 60|    60|   NULL|\n",
      "| 60|    60|Michael|\n",
      "| 60|    80|Michael|\n",
      "| 60|    80|Michael|\n",
      "| 30|    80|   Andy|\n",
      "| 31|    80|   Andy|\n",
      "| 18|    80| Justin|\n",
      "| 19|    80| Justin|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "people_df.fillna(value=60, subset=[\"age\", \"height\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()\n",
    "# people_df.fillna(value=60, subset=[\"age\", \"height\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "|age|height|   name|\n",
      "+---+------+-------+\n",
      "| 50|     0|unknown|\n",
      "| 50|     0|Michael|\n",
      "| 50|    80|Michael|\n",
      "| 50|    80|Michael|\n",
      "| 30|    80|   Andy|\n",
      "| 31|    80|   Andy|\n",
      "| 18|    80| Justin|\n",
      "| 19|    80| Justin|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.fillna({\"age\": 50, \"name\": \"unknown\", \"height\": 0}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.filter(condition: ColumnOrName) → DataFrame\n",
    "\n",
    "Filters rows using the given condition.\n",
    "\n",
    "where() is an alias for filter().\n",
    "\n",
    "### Condition Symbols\n",
    "\n",
    "1. `&` : AND operator\n",
    "2. `|` : OR Operator\n",
    "3. `~` : NOT Operator\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    condition Column or str\n",
    "    a Column of types.BooleanType or a string of SQL expressions.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Filtered DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Column instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(appl_stock_df.Close < 500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by SQL expression in a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(\"Close < 500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering based on multiple conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------+---------+-----------------+---------+-----------------+\n",
      "|      Date|             Open|     High|      Low|            Close|   Volume|        Adj Close|\n",
      "+----------+-----------------+---------+---------+-----------------+---------+-----------------+\n",
      "|2014-06-09|        92.699997|93.879997|    91.75|        93.699997| 75415000|        88.906324|\n",
      "|2014-06-10|        94.730003|95.050003|    93.57|            94.25| 62777000|        89.428189|\n",
      "|2014-06-11|        94.129997|94.760002|93.470001|        93.860001| 45681000|        89.058142|\n",
      "|2014-06-12|        94.040001|94.120003|91.900002|        92.290001| 54749000|        87.568463|\n",
      "|2014-06-13|        92.199997|92.440002|90.879997|        91.279999| 54525000|        86.610132|\n",
      "|2014-06-16|        91.510002|    92.75|91.449997|        92.199997| 35561000|        87.483064|\n",
      "|2014-06-17|        92.309998|92.699997|91.800003|92.08000200000001| 29726000|87.36920699999999|\n",
      "|2014-06-18|        92.269997|92.290001|91.349998|            92.18| 33514000|         87.46409|\n",
      "|2014-06-19|        92.290001|92.300003|91.339996|        91.860001| 35528000|        87.160461|\n",
      "|2014-06-20|        91.849998|92.550003|90.900002|        90.910004|100898000|        86.259066|\n",
      "|2014-06-23|            91.32|91.620003|90.599998|90.83000200000001| 43694000|        86.183157|\n",
      "|2014-06-24|            90.75|91.739998|90.190002|        90.279999| 39036000|        85.661292|\n",
      "|2014-06-25|        90.209999|90.699997|89.650002|        90.360001| 36869000|        85.737201|\n",
      "|2014-06-26|        90.370003|91.050003|89.800003|        90.900002| 32629000|        86.249576|\n",
      "|2014-06-27|            90.82|     92.0|90.769997|        91.980003| 64029000|        87.274325|\n",
      "|2014-06-30|        92.099998|93.730003|92.089996|            92.93| 49482300|         88.17572|\n",
      "|2014-07-01|        93.519997|    94.07|93.129997|        93.519997| 38223000|88.73553199999999|\n",
      "|2014-07-02|        93.870003|94.059998|93.089996|        93.480003| 28465000|        88.697585|\n",
      "|2014-07-03|93.66999799999999|94.099998|93.199997|        94.029999| 22891800|        89.219443|\n",
      "|2014-07-07|        94.139999|95.989998|94.099998|        95.970001| 56468000|        91.060195|\n",
      "+----------+-----------------+---------+---------+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(\"Close < 100 and Open < 200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|        Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "|2012-02-13|        499.529991|503.83000899999996|497.08998899999995|502.60002099999997|129304000|        65.116633|\n",
      "|2012-02-16|        491.500008|        504.890007|         486.62999|502.20999900000004|236138000|        65.066102|\n",
      "|2013-01-16|494.63999900000005|509.44001799999995|492.49997699999994|506.08998099999997|172701200|        66.151072|\n",
      "|2013-01-18|        498.519981|        502.219986|        496.399986|        500.000015|118230700|        65.355052|\n",
      "|2013-10-17|        499.979988|        504.779991|        499.680008|        504.499985| 63398300|        67.207422|\n",
      "|2014-01-31|        495.179985|        501.529984|        493.549988|500.59997599999997|116199300|67.07722700000001|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(\n",
    "    (appl_stock_df[\"Open\"] < 500) & (appl_stock_df[\"Close\"] > 500)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter((appl_stock_df.Open < 500) | ~(appl_stock_df.Close > 500)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = appl_stock_df.filter(\n",
    "    appl_stock_df.Low == 197.16\n",
    ").collect()  # returns results in list of row class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstrow = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date=datetime.date(2010, 1, 22), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401) <class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "print(firstrow, type(firstrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': datetime.date(2010, 1, 22), 'Open': 206.78000600000001, 'High': 207.499996, 'Low': 197.16, 'Close': 197.75, 'Volume': 220441900, 'Adj Close': 25.620401}\n",
      "206.78000600000001\n"
     ]
    }
   ],
   "source": [
    "firstrowasdict = firstrow.asDict()\n",
    "print(firstrowasdict)\n",
    "print(firstrowasdict[\"Open\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.select(\\*cols: ColumnOrName) → DataFrame\n",
    "\n",
    "Projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols str, Column, or list\n",
    "    column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    A DataFrame with subset (or all) of columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|age+10|\n",
      "+-------+----+------+\n",
      "|   NULL|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|   Andy|  30|    40|\n",
      "|   Andy|  31|    41|\n",
      "| Justin|  18|    28|\n",
      "| Justin|  19|    29|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(\n",
    "    people_df.name, people_df.age, (people_df.age + 10).alias(\"age+10\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The column object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'> <class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(people_df.age), type(people_df[\"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|age+10|\n",
      "+-------+----+------+\n",
      "|   NULL|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|   Andy|  30|    40|\n",
      "|   Andy|  31|    41|\n",
      "| Justin|  18|    28|\n",
      "| Justin|  19|    29|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(\n",
    "    people_df[\"name\"], people_df[\"age\"], (people_df[\"age\"] + 10).alias(\"age+10\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumn() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "The column expression must be an expression over this DataFrame; attempting to add a column from some other DataFrame will raise an error.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    colName str\n",
    "    string, name of the new column.\n",
    "\n",
    "    col Column\n",
    "    a Column expression for the new column.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with new or replaced column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+\n",
      "| age|height|   name|age2|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|  30|    80|   Andy|  32|\n",
      "|  31|    80|   Andy|  33|\n",
      "|  18|    80| Justin|  20|\n",
      "|  19|    80| Justin|  21|\n",
      "+----+------+-------+----+\n",
      "\n",
      "+----+------+-------+----+\n",
      "| age|height|   name|age2|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|  30|    80|   Andy|  32|\n",
      "|  31|    80|   Andy|  33|\n",
      "|  18|    80| Justin|  20|\n",
      "|  19|    80| Justin|  21|\n",
      "+----+------+-------+----+\n",
      "\n",
      "+----+------+-------+----+\n",
      "| age|height|   name|age2|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|  30|    80|   Andy|  32|\n",
      "|  31|    80|   Andy|  33|\n",
      "|  18|    80| Justin|  20|\n",
      "|  19|    80| Justin|  21|\n",
      "+----+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumn(colName=\"age2\", col=people_df[\"age\"] + 2).show()\n",
    "people_df.withColumn(colName=\"age2\", col=people_df.age + 2).show()\n",
    "people_df.withColumn(colName=\"age2\", col=col(\"age\") + 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumnRenamed() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by renaming an existing column. This is a no-op if the schema doesn’t contain the given column name.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    existing str\n",
    "    string, name of the existing column to rename.\n",
    "\n",
    "    new str\n",
    "    string, new name of the column.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with renamed column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|newName|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumnRenamed(existing=\"name\", new=\"newName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumns() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by adding multiple columns or replacing the existing columns that have the same names.\n",
    "The colsMap is a map of column name and column, the column must only refer to attributes supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols Mapdict\n",
    "    a dict of column name and Column. Currently, only a single map is supported.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with new or replaced columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+----+\n",
      "| age|height|   name|age2|age3|\n",
      "+----+------+-------+----+----+\n",
      "|NULL|  NULL|   NULL|NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|NULL|\n",
      "|NULL|    80|Michael|NULL|NULL|\n",
      "|NULL|    80|Michael|NULL|NULL|\n",
      "|  30|    80|   Andy|  32|  33|\n",
      "|  31|    80|   Andy|  33|  34|\n",
      "|  18|    80| Justin|  20|  21|\n",
      "|  19|    80| Justin|  21|  22|\n",
      "+----+------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumns({\"age2\": people_df.age + 2, \"age3\": people_df.age + 3}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumnsRenamed() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by renaming multiple columns. This is a no-op if the schema doesn’t contain the given column names.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols Mapdict\n",
    "    a dict of existing column names and corresponding desired column names. Currently, only a single map is supported.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with renamed columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|age4|height|newName|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumnsRenamed({\"age\": \"age4\", \"name\": \"newName\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.groupBy() → GroupedData\n",
    "\n",
    "Groups the DataFrame using the specified columns, so we can run aggregation on them. See GroupedData for all the available aggregate functions.\n",
    "\n",
    "groupby() is an alias for groupBy().\n",
    "\n",
    "`in momory operation`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols list, str or Column\n",
    "    columns to group by. Each element should be a column name (string) or an expression (Column) or list of them.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    GroupedData\n",
    "    Grouped data by given columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedData[grouping expressions: [Company], value: [Company: string, Person: string ... 1 more field], type: GroupBy]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sales_info_df.groupBy(\"Company\"))\n",
    "type(sales_info_df.groupBy(\"Company\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, avg(Sales): double]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_info_df.groupBy(\"Company\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info_df.groupBy(\"Company\").mean().show()\n",
    "sales_info_df.groupBy(\"Company\").min().show()\n",
    "sales_info_df.groupBy(\"Company\").max().show()\n",
    "sales_info_df.groupBy(\"Company\").avg().show()\n",
    "sales_info_df.groupBy(\"Company\").count().show()\n",
    "sales_info_df.groupBy(\"Company\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count_sales|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n",
      "+-----------------+\n",
      "|    average_sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------------------------------------+\n",
      "|format_number(stddev(Sales) AS stddev_sales, 2)|\n",
      "+-----------------------------------------------+\n",
      "|                                         250.09|\n",
      "+-----------------------------------------------+\n",
      "\n",
      "+---------------+\n",
      "|formatted sales|\n",
      "+---------------+\n",
      "|         200.00|\n",
      "|         120.00|\n",
      "|         340.00|\n",
      "|         600.00|\n",
      "|         124.00|\n",
      "|         243.00|\n",
      "|         870.00|\n",
      "|         350.00|\n",
      "|         250.00|\n",
      "|         130.00|\n",
      "|         750.00|\n",
      "|         350.00|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info_df.select(count_distinct(\"Sales\").alias(\"count_sales\")).show()\n",
    "sales_info_df.select(avg(\"Sales\").alias(\"average_sales\")).show()\n",
    "sales_info_df.select(format_number(stddev(\"Sales\").alias(\"stddev_sales\"), 2)).show()\n",
    "\n",
    "\n",
    "sales_info_df.select(format_number(\"sales\", 2).alias(\"formatted sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.orderBy() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame sorted by the specified column(s).\n",
    "\n",
    "`OrderBy is just an alias for the sort function.`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols str, list, or Column, optional\n",
    "    list of Column or column names to sort by.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Sorted DataFrame.\n",
    "\n",
    "### Other Parameters\n",
    "\n",
    "    ascending bool or list, optional, default True\n",
    "    boolean or list of boolean. Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, the length of the list must equal the length of the cols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info_df.orderBy(sales_info_df[\"Sales\"], ascending=True).show()\n",
    "sales_info_df.orderBy(sales_info_df[\"Sales\"].asc()).show()\n",
    "sales_info_df.orderBy(sales_info_df[\"Sales\"].desc()).show()\n",
    "sales_info_df.orderBy(sales_info_df.Sales.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## columns\n",
    "\n",
    "Retrieves the names of all columns in the DataFrame as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'height', 'name']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dtypes\n",
    "\n",
    "Returns all column names and their data types as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('height', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isStreaming\n",
    "\n",
    "Returns True if this DataFrame contains one or more sources that continuously return data as it arrives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema\n",
    "\n",
    "Returns the schema of this DataFrame as a pyspark.sql.types.StructType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('age', LongType(), True), StructField('height', LongType(), True), StructField('name', StringType(), True)])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## na\n",
    "\n",
    "Returns a DataFrameNaFunctions for handling missing values.\n",
    "\n",
    "> class pyspark.sql.DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop([how, thresh, subset]): Returns a new DataFrame omitting rows with null values.\n",
    "\n",
    "1.  howstr, optional ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "2.  thresh: int, optional default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "3.  subset str, tuple or list, optional optional list of column names to consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(\n",
    "    thresh=2\n",
    ").show()  # should have atleast 2 non null values, else drop row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"all\", subset=[\"Name\", \"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"any\", subset=[\"Name\", \"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"any\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill(value[, subset]): Replace null values, alias for na.fill().\n",
    "\n",
    "1.  value int, float, string, bool or dict Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "2.  subset str, tuple or list, optional optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "\n",
    "`Replacement Datatype should be same as the current datatype`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()\n",
    "contains_null_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| 50.0|\n",
      "|emp2| NULL| 50.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| 50.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|        Id|      Name|Sales|\n",
      "+----------+----------+-----+\n",
      "|      emp1|      John| NULL|\n",
      "|      emp2|fill value| NULL|\n",
      "|      emp3|fill value|345.0|\n",
      "|      emp4|     Cindy|456.0|\n",
      "|fill value|fill value| NULL|\n",
      "|      emp5|      emp5|456.0|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(\"fill value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| NULL|\n",
      "|emp2|fill value| NULL|\n",
      "|emp3|fill value|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "|NULL|fill value| NULL|\n",
      "|emp5|      emp5|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(\"fill value\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(Sales)|\n",
      "+----------+\n",
      "|     419.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.select(mean(contains_null_df.Sales)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mean = contains_null_df.select(mean(contains_null_df.Sales)).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|419.0|\n",
      "|emp2| NULL|419.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL|419.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(my_mean, subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|419.0|\n",
      "|emp2| NULL|419.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL|419.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(\n",
    "    contains_null_df.select(mean(contains_null_df.Sales)).collect()[0][0],\n",
    "    subset=[\"Sales\"],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace(to_replace[, value, subset]): Returns a new DataFrame replacing a value with another value.\n",
    "\n",
    "1.  to_replace bool, int, float, string, list or dict Value to be replaced. If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement.\n",
    "\n",
    "2.  value bool, int, float, string or None, optional The replacement value must be a bool, int, float, string or None. If value is a list, value should be of the same length and type as to_replace. If value is a scalar and to_replace is a sequence, then value is used as a replacement for each item in to_replace.\n",
    "\n",
    "3.  subset list, optional optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| Nhoj| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+------+-----+\n",
      "|  Id|  Name|Sales|\n",
      "+----+------+-----+\n",
      "|1pme|  John| NULL|\n",
      "|emp2|  NULL| NULL|\n",
      "|emp3|  NULL|345.0|\n",
      "|emp4|Bhindi|456.0|\n",
      "|NULL|  NULL| NULL|\n",
      "|emp5|  emp5|456.0|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()\n",
    "contains_null_df.na.replace(\"John\", \"Nhoj\").show()\n",
    "contains_null_df.na.replace(\n",
    "    [\"emp1\", \"Cindy\"], [\"1pme\", \"Bhindi\"], subset=[\"Id\", \"Name\"]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing With Dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 4), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appl_stock_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+----+-----+----------+----+\n",
      "|Date      |day_of_month|day_of_year|year|month|weekofyear|hour|\n",
      "+----------+------------+-----------+----+-----+----------+----+\n",
      "|2010-01-04|4           |4          |2010|1    |1         |0   |\n",
      "|2010-01-05|5           |5          |2010|1    |1         |0   |\n",
      "|2010-01-06|6           |6          |2010|1    |1         |0   |\n",
      "|2010-01-07|7           |7          |2010|1    |1         |0   |\n",
      "|2010-01-08|8           |8          |2010|1    |1         |0   |\n",
      "|2010-01-11|11          |11         |2010|1    |2         |0   |\n",
      "|2010-01-12|12          |12         |2010|1    |2         |0   |\n",
      "|2010-01-13|13          |13         |2010|1    |2         |0   |\n",
      "|2010-01-14|14          |14         |2010|1    |2         |0   |\n",
      "|2010-01-15|15          |15         |2010|1    |2         |0   |\n",
      "|2010-01-19|19          |19         |2010|1    |3         |0   |\n",
      "|2010-01-20|20          |20         |2010|1    |3         |0   |\n",
      "|2010-01-21|21          |21         |2010|1    |3         |0   |\n",
      "|2010-01-22|22          |22         |2010|1    |3         |0   |\n",
      "|2010-01-25|25          |25         |2010|1    |4         |0   |\n",
      "|2010-01-26|26          |26         |2010|1    |4         |0   |\n",
      "|2010-01-27|27          |27         |2010|1    |4         |0   |\n",
      "|2010-01-28|28          |28         |2010|1    |4         |0   |\n",
      "|2010-01-29|29          |29         |2010|1    |4         |0   |\n",
      "|2010-02-01|1           |32         |2010|2    |5         |0   |\n",
      "|2010-02-02|2           |33         |2010|2    |5         |0   |\n",
      "|2010-02-03|3           |34         |2010|2    |5         |0   |\n",
      "|2010-02-04|4           |35         |2010|2    |5         |0   |\n",
      "|2010-02-05|5           |36         |2010|2    |5         |0   |\n",
      "|2010-02-08|8           |39         |2010|2    |6         |0   |\n",
      "|2010-02-09|9           |40         |2010|2    |6         |0   |\n",
      "|2010-02-10|10          |41         |2010|2    |6         |0   |\n",
      "|2010-02-11|11          |42         |2010|2    |6         |0   |\n",
      "|2010-02-12|12          |43         |2010|2    |6         |0   |\n",
      "|2010-02-16|16          |47         |2010|2    |7         |0   |\n",
      "|2010-02-17|17          |48         |2010|2    |7         |0   |\n",
      "|2010-02-18|18          |49         |2010|2    |7         |0   |\n",
      "|2010-02-19|19          |50         |2010|2    |7         |0   |\n",
      "|2010-02-22|22          |53         |2010|2    |8         |0   |\n",
      "|2010-02-23|23          |54         |2010|2    |8         |0   |\n",
      "|2010-02-24|24          |55         |2010|2    |8         |0   |\n",
      "|2010-02-25|25          |56         |2010|2    |8         |0   |\n",
      "|2010-02-26|26          |57         |2010|2    |8         |0   |\n",
      "|2010-03-01|1           |60         |2010|3    |9         |0   |\n",
      "|2010-03-02|2           |61         |2010|3    |9         |0   |\n",
      "|2010-03-03|3           |62         |2010|3    |9         |0   |\n",
      "|2010-03-04|4           |63         |2010|3    |9         |0   |\n",
      "|2010-03-05|5           |64         |2010|3    |9         |0   |\n",
      "|2010-03-08|8           |67         |2010|3    |10        |0   |\n",
      "|2010-03-09|9           |68         |2010|3    |10        |0   |\n",
      "|2010-03-10|10          |69         |2010|3    |10        |0   |\n",
      "|2010-03-11|11          |70         |2010|3    |10        |0   |\n",
      "|2010-03-12|12          |71         |2010|3    |10        |0   |\n",
      "|2010-03-15|15          |74         |2010|3    |11        |0   |\n",
      "|2010-03-16|16          |75         |2010|3    |11        |0   |\n",
      "+----------+------------+-----------+----+-----+----------+----+\n",
      "only showing top 50 rows\n",
      "\n",
      "+-------------+-------------+\n",
      "|get_only_year|average_close|\n",
      "+-------------+-------------+\n",
      "|         2016|       104.60|\n",
      "|         2015|       120.04|\n",
      "|         2010|       259.84|\n",
      "|         2014|       295.40|\n",
      "|         2011|       364.00|\n",
      "|         2013|       472.63|\n",
      "|         2012|       576.05|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.select(\n",
    "    col(\"Date\"),\n",
    "    dayofmonth(col(\"Date\")).alias(\"day_of_month\"),\n",
    "    dayofyear(col(\"Date\")).alias(\"day_of_year\"),\n",
    "    year(col(\"Date\")).alias(\"year\"),\n",
    "    month(col(\"Date\")).alias(\"month\"),\n",
    "    weekofyear(col(\"Date\")).alias(\"weekofyear\"),\n",
    "    hour(col(\"Date\")).alias(\"hour\"),\n",
    ").show(50, truncate=False)\n",
    "\n",
    "appl_stock_df.withColumn(\"get_only_year\", year(appl_stock_df.Date)).groupBy(\n",
    "    \"get_only_year\"\n",
    ").mean().select(\n",
    "    col(\"get_only_year\"), format_number(col(\"avg(Close)\"), 2).alias(\"average_close\")\n",
    ").orderBy(\n",
    "    \"average_close\", ascending=True\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitions and Partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark.repartition(num_partitions: int) → ps.DataFrame\n",
    "\n",
    "Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    numPartitions int\n",
    "    can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "    cols str or Column\n",
    "    partitioning columns.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    Repartitioned DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0| 1762|\n",
      "+------------------+-----+\n",
      "\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0|  105|\n",
      "|                 1|   87|\n",
      "|                 2|   73|\n",
      "|                 3|   82|\n",
      "|                 4|   80|\n",
      "|                 5|   80|\n",
      "|                 6|   80|\n",
      "|                 7|  115|\n",
      "|                 8|   72|\n",
      "|                 9|   88|\n",
      "|                10|   77|\n",
      "|                11|   96|\n",
      "|                12|   85|\n",
      "|                13|  104|\n",
      "|                14|   84|\n",
      "|                15|   96|\n",
      "|                16|   76|\n",
      "|                17|   97|\n",
      "|                18|   89|\n",
      "|                19|   96|\n",
      "+------------------+-----+\n",
      "\n",
      "20\n",
      "+------------+-----+\n",
      "|my_part_year|count|\n",
      "+------------+-----+\n",
      "|        2015|  252|\n",
      "|        2013|  252|\n",
      "|        2014|  252|\n",
      "|        2012|  250|\n",
      "|        2016|  252|\n",
      "|        2010|  252|\n",
      "|        2011|  252|\n",
      "+------------+-----+\n",
      "\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0|  250|\n",
      "|                 1|  252|\n",
      "|                 3|  252|\n",
      "|                 5|  756|\n",
      "|                 6|  252|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(appl_stock_df.rdd.getNumPartitions())\n",
    "get_records_per_partition(appl_stock_df).show()\n",
    "\n",
    "\n",
    "partitioned_appl_stock_df = appl_stock_df.withColumn(\"my_part_year\", year(col(\"Date\")))\n",
    "partitioned_appl_stock_df_1 = partitioned_appl_stock_df.repartition(\n",
    "    20, \"my_part_year\", \"Close\"\n",
    ")\n",
    "\n",
    "get_records_per_partition(partitioned_appl_stock_df_1).show()\n",
    "print(partitioned_appl_stock_df_1.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "unique_year_df = partitioned_appl_stock_df.groupBy(\"my_part_year\").count()\n",
    "unique_year_df.show()\n",
    "\n",
    "part_count = unique_year_df.count()\n",
    "\n",
    "\n",
    "partitioned_appl_stock_df_2 = partitioned_appl_stock_df.repartition(\n",
    "    part_count, \"my_part_year\"\n",
    ")\n",
    "\n",
    "get_records_per_partition(partitioned_appl_stock_df_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.repartitionByRange() → DataFrame\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "    numPartitions int\n",
    "    can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "    cols str or Column\n",
    "    partitioning columns.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Repartitioned DataFrame.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "At least one partition-by expression must be specified. When no explicit sort order is specified, “ascending nulls first” is assumed.\n",
    "\n",
    "Due to performance reasons this method uses sampling to estimate the ranges. Hence, the output may not be consistent, since sampling can return different values. The sample size can be controlled by the config spark.sql.execution.rangeExchange.sampleSizePerPartition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0| 1762|\n",
      "+------------------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|my_part_year|count|\n",
      "+------------+-----+\n",
      "|        2015|  252|\n",
      "|        2013|  252|\n",
      "|        2014|  252|\n",
      "|        2012|  250|\n",
      "|        2016|  252|\n",
      "|        2010|  252|\n",
      "|        2011|  252|\n",
      "+------------+-----+\n",
      "\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0|  252|\n",
      "|                 1|  252|\n",
      "|                 2|  502|\n",
      "|                 3|  252|\n",
      "|                 4|  252|\n",
      "|                 5|  252|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(appl_stock_df.rdd.getNumPartitions())\n",
    "get_records_per_partition(appl_stock_df).show()\n",
    "\n",
    "\n",
    "unique_year_df = partitioned_appl_stock_df.groupBy(\"my_part_year\").count()\n",
    "unique_year_df.show()\n",
    "\n",
    "part_count = unique_year_df.count()\n",
    "\n",
    "\n",
    "partitioned_appl_stock_df_2 = partitioned_appl_stock_df.repartitionByRange(\n",
    "    part_count, \"my_part_year\"\n",
    ")\n",
    "\n",
    "get_records_per_partition(partitioned_appl_stock_df_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between the two methods\n",
    "\n",
    "In Apache Spark, both repartition and repartitionByRange are methods used for controlling the partitioning of data in a DataFrame, but they differ in their approaches and use cases:\n",
    "\n",
    "| Parameter | repartition                                                                                                                                                                                    | repartitionByRange                                                                                                                                                                                                                                                                               |\n",
    "| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Purpose   | The repartition method is used to increase or decrease the number of partitions in a DataFrame by performing a full shuffle of the data.                                                       | The repartitionByRange method is used for range-based repartitioning, where data is partitioned based on the values within a specified range of a particular column.                                                                                                                             |\n",
    "| Behavior  | The data is randomly redistributed across the specified number of partitions. This method is often used to achieve a more balanced data distribution or to increase parallelism in Spark jobs. | The data is repartitioned based on the specified column and is sorted within each partition according to the values in that column. This is useful for scenarios where certain operations, such as range queries or joins, can benefit from having related data colocated in the same partition. |\n",
    "| Use Case  | Use repartition when you want to change the number of partitions without considering the data distribution based on the column values.                                                         | Use repartitionByRange when you want to optimize specific operations, like range-based queries or joins, by ensuring that data with similar values in a particular column is colocated in the same partition.                                                                                    |\n",
    "\n",
    "In summary, while both repartition and repartitionByRange are used to control data partitioning in Spark, repartition is a more general method that shuffles data randomly across partitions, and repartitionByRange is a more specialized method that organizes data based on the range of values in a specific column. The choice between them depends on the use case and the specific requirements of your Spark job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class pyspark.sql.DataFrameWriter(df: DataFrame)\n",
    "\n",
    "Interface used to write a DataFrame to external storage systems (e.g. file systems, key-value stores, etc). Use DataFrame.write to access this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameWriter.bucketBy() → pyspark.sql.readwriter.DataFrameWriter\n",
    "\n",
    "Buckets the output by the given columns. If specified, the output is laid out on the file system similar to Hive’s bucketing scheme, but with a different bucket hash function and is not compatible with Hive’s bucketing.\n",
    "\n",
    "In the context of Apache Spark, bucketBy is a method that can be used to organize data in DataFrames into buckets or partitions based on a specified column or set of columns. This is often used in conjunction with the saveAsTable method to save the DataFrame as a table in a structured storage system, such as Apache Hive.\n",
    "\n",
    "Here's a brief overview of what happens when you save a DataFrame as a table using bucketBy:\n",
    "\n",
    "1. `Bucketing`: The bucketBy method is used to specify the columns based on which the DataFrame should be bucketed. Bucketing is a way to distribute data across a fixed number of buckets or partitions. It is commonly used to improve query performance, especially for certain types of joins and aggregations.\n",
    "   `df.write.bucketBy(\"column_name\", numBuckets).saveAsTable(\"table_name\")`\n",
    "   In the above code snippet, replace \"column_name\" with the actual column name you want to use for bucketing, \"numBuckets\" with the desired number of buckets, and \"table_name\" with the name of the table you want to create.\n",
    "2. `Saving as a Table`: After bucketing, the saveAsTable method is used to persist the DataFrame as a table in a structured storage system. This could be Hive, which is commonly used with Spark, or another system compatible with the Spark SQL interface.\n",
    "3. `Metadata`: When you save a DataFrame as a table, metadata about the table, such as the schema and bucketing information, is stored. This metadata is important for query optimization and execution.\n",
    "\n",
    "4. `Query Optimization`: Bucketing can lead to more efficient query processing, especially for operations like joins and aggregations. When querying the table, Spark can leverage the knowledge of the bucketing scheme to skip unnecessary data shuffling and reduce the amount of data that needs to be read.\n",
    "\n",
    "It's important to note that the effectiveness of bucketing depends on the nature of your data and the types of queries you frequently run. It might not always lead to performance improvements, and in some cases, it could even introduce overhead. Always consider your specific use case and data distribution when deciding whether to use bucketing.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "    numBuckets int\n",
    "    the number of buckets to save\n",
    "\n",
    "    col str, list or tuple\n",
    "    a name of a column, or a list of names.\n",
    "\n",
    "    cols str\n",
    "    additional names (optional). If col is a list it should be empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_stock_df.withColumn(\"my_part_year\", year(col(\"Date\"))).write.bucketBy(\n",
    "    7, \"my_part_year\"\n",
    ").mode(\"overwrite\").saveAsTable(\"appl_stock_df_bucketed_table_by_year\")\n",
    "appl_stock_df.write.bucketBy(20, \"Date\").mode(\"overwrite\").saveAsTable(\n",
    "    \"appl_stock_df_bucketed_table\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|my_part_year|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+------------+\n",
      "|2011-01-03|        325.640003|        330.260002|        324.840012|            329.57|111284600|         42.698941|        2011|\n",
      "|2011-01-04|        332.439999|             332.5|        328.149994|        331.290012| 77270200|         42.921785|        2011|\n",
      "|2011-01-05|        329.549999|        334.339989|        329.500011|        334.000008| 63879900|43.272890999999994|        2011|\n",
      "|2011-01-06|334.71999700000003|        335.249996|        332.900009|        333.729988| 75107200|         43.237907|        2011|\n",
      "|2011-01-07|333.98999399999997|336.34999500000004|        331.900013|        336.120003| 77982800|         43.547557|        2011|\n",
      "|2011-01-10|        338.829998|        343.229992|        337.169987|        342.450001|112140000|         44.367668|        2011|\n",
      "|2011-01-11|344.87998999999996|        344.959991|        339.470013|        341.639996|111027000|         44.262724|        2011|\n",
      "|2011-01-12|        343.249992|        344.429993|        342.000004|        344.420006| 75647600|         44.622901|        2011|\n",
      "|2011-01-13|        345.159996|        346.640003|        343.850006|        345.680008| 74195100|         44.786147|        2011|\n",
      "|2011-01-14|345.88999900000005|        348.479992|        344.440006|        348.479992| 77210000|         45.148911|        2011|\n",
      "|2011-01-18|        329.520012|        344.759987|        326.000011|        340.650013|470249500|44.134463000000004|        2011|\n",
      "|2011-01-19|        348.350002|348.59999500000004|        336.879993|        338.840012|283903200|          43.89996|        2011|\n",
      "|2011-01-20|        336.429996|        338.299999|        330.119999|        332.680004|191197300|         43.101872|        2011|\n",
      "|2011-01-21|        333.769989|        334.880001|326.63001299999996|326.72000099999997|188600300|42.329696999999996|        2011|\n",
      "|2011-01-24|        326.869991|        337.449993|326.72000099999997|        337.449993|143670800|          43.71987|        2011|\n",
      "|2011-01-25|        336.329994|        341.439991|        334.570007|         341.39999|136717000|         44.231629|        2011|\n",
      "|2011-01-26|342.95999900000004|        345.600006|        341.499992|        343.850006|126718900|         44.549052|        2011|\n",
      "|2011-01-27|        343.779991|        344.689999|342.83000899999996|        343.209991| 71256500|         44.466132|        2011|\n",
      "|2011-01-28|        344.169987|344.40000499999996|         333.53001|        336.100002|148014300|43.544965000000005|        2011|\n",
      "|2011-01-31|        335.799995|        340.040012|        334.299988|        339.319996| 94311700|43.962146000000004|        2011|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"appl_stock_df_bucketed_table_by_year\").sortWithinPartitions(\n",
    "    \"my_part_year\"\n",
    ").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0|  756|\n",
      "|                 1|  252|\n",
      "|                 2|  250|\n",
      "|                 3|  252|\n",
      "|                 4|  252|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_records_per_partition(\n",
    "    spark.read.table(\"appl_stock_df_bucketed_table_by_year\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-02-08|        195.690006|197.88000300000002|        193.999994|194.11999699999998|119567700|           25.1501|\n",
      "|2010-04-07|239.54999500000002|241.92001000000002|        238.659988|        240.600006|157125500|         31.172029|\n",
      "|2010-06-25|270.06001299999997|270.27000400000003|        265.810009|        266.699989|137485600|         34.553531|\n",
      "|2010-08-31|        241.849995|         244.55999|        240.349987|         243.10001|105196700|31.495928000000003|\n",
      "|2011-09-28|400.18998700000003|        403.740002|         396.51001|        397.009995|107409400|         51.436437|\n",
      "|2012-01-26|        448.360008|        448.789978|        443.139996|         444.62999| 80996300|         57.606062|\n",
      "|2012-02-23|        515.079987|        517.830009|        509.499992| 516.3899769999999|142006900|         66.903253|\n",
      "|2012-04-05|        626.980011| 634.6600269999999|        623.400009|        633.679977|160324500|         82.099293|\n",
      "|2012-06-13| 574.5200120000001|        578.479996|        570.379997|        572.160011| 73395000|         74.128794|\n",
      "|2012-08-30| 670.6400219999999| 671.5500030000001|        662.849991|        663.869987| 75674900| 86.37998499999999|\n",
      "|2013-04-16|        421.569996|        426.610004|            420.57|        426.240009| 76442800|         56.038576|\n",
      "|2013-06-17|        431.439995|        435.700012|430.35999699999996|        432.000008| 64853600|         57.171785|\n",
      "|2013-12-13|        562.849998|        562.880013|        553.669975|        554.429993| 83205500| 74.29010799999999|\n",
      "|2014-09-15|        102.809998|        103.050003|        101.440002|        101.629997| 61316500| 96.91028100000001|\n",
      "|2014-11-26|        117.940002|        119.099998|117.83000200000001|             119.0| 40768300|113.96566299999999|\n",
      "|2015-05-18|        128.380005|        130.720001|        128.360001|        130.190002| 50882900|        125.697198|\n",
      "|2015-06-05|             129.5|        129.690002|        128.360001|        128.649994| 35626800|        124.210334|\n",
      "|2016-06-22|             96.25|         96.889999|         95.349998|         95.550003| 29219100|          94.14158|\n",
      "|2016-11-25|        111.129997|        111.870003|        110.949997|        111.790001| 11475900|        111.307418|\n",
      "|2016-11-30|        111.599998|        112.199997|        110.269997|        110.519997| 36162300|        110.042897|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"appl_stock_df_bucketed_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE appl_stock_df_bucketed_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameWriter.mode(saveMode: Optional[str]) → pyspark.sql.readwriter.DataFrameWriter[source]\n",
    "\n",
    "Specifies the behavior when data or table already exists.\n",
    "\n",
    "Options include:\n",
    "\n",
    "1. append: Append contents of this DataFrame to existing data.\n",
    "2. overwrite: Overwrite existing data.\n",
    "3. error or errorifexists: Throw an exception if data already exists.\n",
    "4. ignore: Silently ignore this operation if data already exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output files\n",
    "\n",
    "1. \\_SUCCESS File: This file is an empty file that serves as a marker to indicate that the write operation completed successfully. It doesn't contain any data but is often used as a signal for downstream processes or job monitoring tools.\n",
    "2. \\_temporary Directory: Spark may create a temporary directory (e.g., \\_temporary) during the write process. This directory is used to stage data before it is moved to the final destination. Once the write operation is successful, the data is moved from the temporary directory to the final output directory.\n",
    "3. Hidden Files (e.g., .part-xxxxx): These files are temporary files generated during the write process. They represent different partitions of the data and are merged or moved to the final output directory upon successful completion of the write operation.\n",
    "4. _committed_ Directory (for Delta Lake): If you are using Delta Lake to write your DataFrame, you might see a directory named _committed_. This directory contains committed Delta Lake transactions and is part of the Delta Lake transaction log.\n",
    "\n",
    "CRC files, or Cyclic Redundancy Check files, are checksum files used to verify the integrity of data. In the context of Spark and distributed file systems, CRC files are often associated with the process of writing data to ensure data consistency and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Formats\n",
    "\n",
    "1. csv(path[, mode, compression, sep, quote, …]): Saves the content of the DataFrame in CSV format at the specified path.\n",
    "2. json(path[, mode, compression, dateFormat, …]): Saves the content of the DataFrame in JSON format (JSON Lines text format or newline-delimited JSON) at the specified path.\n",
    "3. orc(path[, mode, partitionBy, compression]): Saves the content of the DataFrame in ORC format at the specified path.\n",
    "4. parquet(path[, mode, partitionBy, compression]): Saves the content of the DataFrame in Parquet format at the specified path.\n",
    "5. text(path[, compression, lineSep]): Saves the content of the DataFrame in a text file at the specified path. `The DataFrame must have only one column that is of string type. Each row becomes a new line in the output file.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Compression Moethods while saving files\n",
    "\n",
    "#### Parquet:\n",
    "\n",
    "Parquet is a columnar storage format and is the default choice for Spark's DataFrame API.\n",
    "\n",
    "1. Snappy: Compression algorithm that provides a good balance between compression ratio and speed. Use snappy as the compression codec.\n",
    "   `df.write.parquet(\"/path/to/output\").option(\"compression\", \"snappy\").save()`\n",
    "2. Gzip: Provides higher compression but can be slower. Use gzip as the compression codec.\n",
    "   `df.write.parquet(\"/path/to/output\").option(\"compression\", \"gzip\").save()`\n",
    "\n",
    "#### ORC:\n",
    "\n",
    "ORC (Optimized Row Columnar) is another columnar storage format.\n",
    "\n",
    "1. Snappy: Similar to Parquet, you can use snappy as the compression codec.\n",
    "   `df.write.orc(\"/path/to/output\").option(\"compression\", \"snappy\").save()`\n",
    "2. Zlib: Another compression option for ORC is zlib.\n",
    "   `df.write.orc(\"/path/to/output\").option(\"compression\", \"zlib\").save()`\n",
    "\n",
    "#### Avro:\n",
    "\n",
    "Avro is a binary serialization format.\n",
    "\n",
    "1. Deflate: Avro supports deflate compression.\n",
    "   `df.write.format(\"avro\").option(\"compression\", \"deflate\").save(\"/path/to/output\")`\n",
    "\n",
    "#### CSV:\n",
    "\n",
    "For CSV files, compression is often applied to the entire output directory rather than individual files. You can use Hadoop codecs for compression.\n",
    "\n",
    "1. Gzip:\n",
    "   `df.write.option(\"compression\", \"gzip\").csv(\"/path/to/output\")`\n",
    "2. Bzip2:\n",
    "   `df.write.option(\"compression\", \"bzip2\").csv(\"/path/to/output\")`\n",
    "\n",
    "#### Delta Lake:\n",
    "\n",
    "Delta Lake, being a storage layer for Spark, has its own compression options.\n",
    "\n",
    "1. Snappy: Delta Lake supports snappy compression.\n",
    "   `df.write.format(\"delta\").option(\"compression\", \"snappy\").save(\"/path/to/output\")`\n",
    "2. Zstd: Delta Lake also supports zstd compression.\n",
    "   `df.write.format(\"delta\").option(\"compression\", \"zstd\").save(\"/path/to/output\")`\n",
    "\n",
    "#### Compression Flags\n",
    "\n",
    "1. Snappy:\n",
    "   Description: Snappy is a fast compression and decompression algorithm designed for speed rather than high compression ratios.\n",
    "   Use Case: It is suitable for scenarios where low-latency and fast compression/decompression are more critical than achieving the maximum compression ratio.\n",
    "   Pros: Fast compression and decompression, low overhead.\n",
    "   Cons: Lower compression ratios compared to some other algorithms.\n",
    "2. Gzip:\n",
    "   Description: Gzip is a widely used general-purpose compression algorithm that provides a good balance between compression ratio and speed.\n",
    "   Use Case: It is suitable for scenarios where moderate compression is needed with a reasonable trade-off in terms of speed.\n",
    "   Pros: Good compression ratios, widely supported.\n",
    "   Cons: Slower compression and decompression compared to Snappy.\n",
    "3. Zlib:\n",
    "   Description: Zlib is a software library used for data compression. It is often associated with the DEFLATE compression algorithm.\n",
    "   Use Case: It is similar to Gzip and can be used when you need compatibility with systems that support zlib compression.\n",
    "   Pros: Good compression ratios, widely supported.\n",
    "   Cons: Slower compression and decompression compared to Snappy.\n",
    "4. Deflate (Avro):\n",
    "   Description: Deflate is a compression algorithm that uses a combination of LZ77 and Huffman coding.\n",
    "   Use Case: It is commonly used with Avro files.\n",
    "   Pros: Good compression ratios.\n",
    "   Cons: Can be slower compared to Snappy, and Avro's native support is limited to Deflate.\n",
    "5. Bzip2 (CSV):\n",
    "   Description: Bzip2 is a compression algorithm known for its high compression ratios.\n",
    "   Use Case: It can be used when achieving maximum compression is a priority, and speed is less critical.\n",
    "   Pros: Excellent compression ratios.\n",
    "   Cons: Slower compression and decompression compared to algorithms like Snappy.\n",
    "6. Zstd (Delta Lake):\n",
    "   Description: Zstd (Zstandard) is a modern compression algorithm designed for high compression ratios and fast compression/decompression.\n",
    "   Use Case: It is suitable for scenarios where a good balance between compression ratios and speed is desired.\n",
    "   Pros: Good compression ratios, fast compression and decompression.\n",
    "   Cons: May have slightly higher overhead compared to very fast algorithms like Snappy.\n",
    "\n",
    "#### Choosing the \"Best\" Compression Format:\n",
    "\n",
    "Trade-offs: The choice of the best compression format depends on the specific use case and the trade-offs between compression ratios, speed, and compatibility.\n",
    "Use Case Specific: Consider factors such as the nature of your data, the read and write patterns of your workload, and the resources available in your computing environment.\n",
    "Compatibility: Choose a compression format that is widely supported by the systems and tools you are using in your data processing pipeline.\n",
    "In summary, there is no one-size-fits-all answer to the \"best\" compression format. It often involves a trade-off between compression ratios and speed based on your specific requirements and constraints. Consider conducting performance tests with your data to determine the compression format that works best for your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_stock_df.coalesce(1).write.mode(\"overwrite\").csv(\"./output/appl_stock_csv\")\n",
    "appl_stock_df.write.mode(\"overwrite\").json(\"./output/appl_stock_json\")\n",
    "appl_stock_df.write.mode(\"overwrite\").orc(\"./output/appl_stock_orc\")\n",
    "appl_stock_df.write.mode(\"overwrite\").parquet(\"./output/appl_stock_parquet\")\n",
    "people_df.drop(\"age\", \"height\").write.mode(\"overwrite\").text(\"./output/people_df_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing vs Repartioning\n",
    "\n",
    "| Parameter | Bucketing                                                                                                                                                                                                                            | Repartioning                                                                                                                                                                                                                                                                                                               |\n",
    "| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Purpose   | Bucketing is a technique used for organizing data into a fixed number of buckets based on the values of one or more columns. It is mainly employed for optimizing certain types of operations, like joins and aggregations.          | Repartitioning is the process of redistributing data across the partitions in a DataFrame or RDD (Resilient Distributed Dataset).                                                                                                                                                                                          |\n",
    "| Use Case  | Bucketing is useful when you want to optimize the performance of certain queries by ensuring that related data is colocated in the same bucket. This reduces the amount of data that needs to be shuffled during certain operations. | It is typically used to control the parallelism of Spark jobs and optimize data distribution for better performance. For example, if you have too few partitions, you might not fully utilize the available resources, while too many partitions can lead to excessive overhead due to task scheduling and data shuffling. |\n",
    "| Method    | You use the bucketBy method when writing data and the bucket function when reading data to specify bucketing. Additionally, you can set the number of buckets with the numBuckets parameter.                                         | You can use the repartition or coalesce methods in Spark to repartition your data. The former performs a full shuffle, while the latter tries to minimize data movement.                                                                                                                                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Transformations vs Actions\n",
    "\n",
    "**Here's a tabular explanation of Transformations and Actions in Apache Spark, formatted in Markdown:**\n",
    "\n",
    "| Feature               | Transformations                                                | Actions                                                                        |\n",
    "| --------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Definition**        | Operations that create new RDDs from existing ones.            | Operations that trigger computations and return results to the driver program. |\n",
    "| **Lazy Evaluation**   | Yes, only define a lineage of operations.                      | No, trigger actual computations.                                               |\n",
    "| **Return Value**      | A new RDD.                                                     | A value or write data to storage.                                              |\n",
    "| **Examples**          | map, filter, flatMap, groupByKey, reduceByKey, sortByKey, etc. | count, collect, take, reduce, first, saveAsTextFile, etc.                      |\n",
    "| **Trigger Execution** | No, only define a computation graph.                           | Yes, initiate the execution of the DAG.                                        |\n",
    "| **Immutability**      | Maintain immutability of RDDs.                                 | Create new data structures for results.                                        |\n",
    "| **Partitioning**      | Can affect partitioning of output RDDs.                        | May require data shuffling for aggregations.                                   |\n",
    "| **Optimization**      | Spark optimizes transformations for efficient execution.       | Spark optimizes action execution based on the defined transformations.         |\n",
    "\n",
    "**Additional Points:**\n",
    "\n",
    "- **Narrow vs. Wide Transformations:**\n",
    "\n",
    "  - **Narrow:** Transformations where each partition of the output RDD depends on only one partition of the input RDD (e.g., map, filter).\n",
    "  - **Wide:** Transformations where each partition of the output RDD may depend on multiple partitions of the input RDD (e.g., groupByKey, reduceByKey).\n",
    "\n",
    "- **Understanding DAGs:**\n",
    "  - Transformations create a Directed Acyclic Graph (DAG) of operations.\n",
    "  - Actions trigger the execution of the DAG.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Transformations build up a computation plan, while actions execute it.\n",
    "- Lazy evaluation allows Spark to optimize execution before triggering computations.\n",
    "- Understanding transformations and actions is essential for efficient Spark programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark Concepts Overview\n",
    "\n",
    "#### DAG (Directed Acyclic Graph):\n",
    "\n",
    "A **Directed Acyclic Graph (DAG)** is a representation of the logical execution plan in Apache Spark. It's a graph where nodes represent operations, and edges define the flow of data between these operations. Spark uses the DAG to optimize the execution plan and efficiently schedule tasks across a distributed cluster. It is divided into stages, where each stage contains a set of tasks that can be executed in parallel.\n",
    "\n",
    "#### Catalyst:\n",
    "\n",
    "**Catalyst** is the query optimization framework in Apache Spark. It employs a rule-based optimization strategy to optimize both logical and physical execution plans of Spark SQL queries. Catalyst can push down predicates, project only necessary columns, and perform various optimizations to enhance the performance of Spark jobs.\n",
    "\n",
    "#### Tungsten Binary Format:\n",
    "\n",
    "**Tungsten Binary Format** is a columnar, in-memory data representation in Apache Spark designed for optimizing storage and processing. It employs binary encoding and compression techniques to reduce memory consumption and accelerate query execution. Tungsten is part of the execution engine and focuses on efficient, in-memory processing.\n",
    "\n",
    "#### Shuffles:\n",
    "\n",
    "**Shuffling** is the process of redistributing data across partitions of a Spark RDD or DataFrame. It's a resource-intensive operation that involves data exchange between nodes in a Spark cluster. Shuffling is typically required during certain transformations like groupBy or join, where data needs to be reorganized across partitions.\n",
    "\n",
    "#### Data Skew:\n",
    "\n",
    "**Data Skew** refers to an uneven distribution of data across partitions during shuffling. If certain keys have significantly more data than others, it can lead to imbalanced workloads and performance issues. Strategies to address data skew include using appropriate partitioning techniques, employing efficient join algorithms, or repartitioning the data.\n",
    "\n",
    "#### Spark Job Execution:\n",
    "\n",
    "1. **Transformation Commands:**\n",
    "\n",
    "   - Spark transformations (e.g., map, filter) are lazily evaluated, meaning they create a logical execution plan but don't execute immediately.\n",
    "\n",
    "2. **Action Commands:**\n",
    "\n",
    "   - When an action command (e.g., `collect`, `count`) is invoked, Spark initiates the execution. The logical plan is optimized using Catalyst and translated into a physical execution plan.\n",
    "\n",
    "3. **Task Execution:**\n",
    "\n",
    "   - The physical execution plan is divided into stages, each containing tasks. Tasks are distributed to worker nodes in the Spark cluster for parallel processing.\n",
    "\n",
    "4. **Data Processing:**\n",
    "\n",
    "   - Tasks process data in parallel on worker nodes, utilizing Tungsten for optimized in-memory processing.\n",
    "\n",
    "5. **Shuffling:**\n",
    "\n",
    "   - If shuffling is required, data is exchanged between partitions to ensure co-location for further processing.\n",
    "\n",
    "6. **Output:**\n",
    "   - The final results are collected or saved based on the action command. The output is displayed or stored as specified.\n"
   ]
  },
  {
   "attachments": {
    "Screenshot 2023-11-20 173308.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2023-11-20 173308.png](<attachment:Screenshot 2023-11-20 173308.png>)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
