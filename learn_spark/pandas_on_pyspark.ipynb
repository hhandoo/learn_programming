{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as pypn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = spark.sql(\"select 'spark' as hello;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a csv in Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\development\\learn_spark\\.venv\\Lib\\site-packages\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "psdf = pypn.read_csv(\"./datasets/appl_stock_w_id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id        Date        Open        High         Low       Close     Volume  Adj Close\n",
      "0   1  04-01-2010  213.429998  214.499996  212.380001  214.009998  123432400  27.727039\n",
      "1   2  05-01-2010  214.599998  215.589994  213.249994  214.379993  150476200  27.774976\n",
      "2   3  06-01-2010  214.379993  215.230000  210.750004  210.969995  138040000  27.333178\n",
      "3   4  07-01-2010  211.750000  212.000006  209.050005  210.580000  119282800  27.282650\n",
      "4   5  08-01-2010  210.299994  212.000006  209.060005  211.980005  111902700  27.464034\n"
     ]
    }
   ],
   "source": [
    "print(psdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.pandas.frame.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(psdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                 | Pandas Core DataFrame                                | Pandas on PySpark DataFrame                                | PySpark DataFrame                                          |\n",
    "| ----------------------- | ---------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Language**            | Python                                               | Python                                                     | Primarily Scala (with APIs in Java, Python, R)             |\n",
    "| **Deployment**          | Single-node, in-memory computation                   | Distributed, scalable to clusters                          | Distributed, scalable to clusters                          |\n",
    "| **Performance**         | Optimized for single-node computation                | Scales horizontally for large datasets                     | Scales horizontally for large datasets                     |\n",
    "| **Data Size**           | Best for small to medium-sized datasets              | Handles large-scale distributed data                       | Handles large-scale distributed data                       |\n",
    "| **Ease of Use**         | Extremely user-friendly, simple API                  | Slightly more complex due to distributed nature            | Learning curve for Spark's architecture                    |\n",
    "| **Installation**        | Easy to install and set up                           | Requires Apache Spark installation                         | Requires Apache Spark installation                         |\n",
    "| **Data Sources**        | Reads from various sources                           | Connects to distributed data sources                       | Connects to distributed data sources                       |\n",
    "| **Operations**          | In-memory operations                                 | Transformation and action operations for distributed data  | Transformation and action operations for distributed data  |\n",
    "| **Parallel Processing** | Limited parallelism on a single machine              | Utilizes Spark's built-in parallel processing              | Utilizes Spark's built-in parallel processing              |\n",
    "| **Scale Out**           | Vertical scaling (upgrading hardware)                | Horizontal scaling (adding more nodes to the cluster)      | Horizontal scaling (adding more nodes to the cluster)      |\n",
    "| **Integration**         | Integrates seamlessly with Python ecosystem          | Integrates with PySpark and Spark ecosystem                | Integrates with PySpark and Spark ecosystem                |\n",
    "| **Machine Learning**    | Integrates with scikit-learn, TensorFlow, etc.       | Includes MLlib for distributed machine learning            | Includes MLlib for distributed machine learning            |\n",
    "| **Community Support**   | Large and well-established community support         | Active open-source community support for PySpark           | Active open-source community support for PySpark           |\n",
    "| **Use Cases**           | Data analysis, EDA, small to medium-scale processing | Big data processing, distributed computing, data pipelines | Big data processing, distributed computing, data pipelines |\n",
    "\n",
    "Note: The choice between these DataFrame options depends on the specific use case, data size, and whether distributed computing capabilities are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get undistributed pandas dataframe from pandas on spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\development\\learn_spark\\.venv\\Lib\\site-packages\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "pdf = psdf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.pandas.frame.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(psdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
