{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    substring,\n",
    "    input_file_name,\n",
    "    current_date,\n",
    "    year,\n",
    "    like,\n",
    "    asc,\n",
    "    desc,\n",
    "    count,\n",
    "    count_distinct,\n",
    "    max,\n",
    "    min,\n",
    "    avg,\n",
    "    array_agg,\n",
    "    array_distinct,\n",
    "    array_contains,\n",
    "    row_number,\n",
    "    rank,\n",
    "    dense_rank,\n",
    "    ntile,\n",
    "    sum,\n",
    "    lead,\n",
    "    lag,\n",
    "    cume_dist,\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.config(\n",
    "    \"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"\n",
    ").config(\n",
    "    \"spark.sql.catalog.spark_catalog\",\n",
    "    \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data into spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_user_schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), nullable=False),\n",
    "        StructField(\"organization_id\", StringType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=False),\n",
    "        StructField(\"website\", StringType(), nullable=False),\n",
    "        StructField(\"country\", StringType(), nullable=False),\n",
    "        StructField(\"description\", StringType(), nullable=False),\n",
    "        StructField(\"founded\", IntegerType(), nullable=False),\n",
    "        StructField(\"industry\", StringType(), nullable=False),\n",
    "        StructField(\"employee_no\", IntegerType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "users_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .schema(my_user_schema)\n",
    "    .csv(\"./datasets/organizations-2000000.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "click_data_schema = StructType(\n",
    "    [\n",
    "        StructField(\"session_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"IPID\", IntegerType(), nullable=False),\n",
    "        StructField(\"timestamp\", TimestampType(), nullable=False),\n",
    "        StructField(\"VHOST\", StringType(), nullable=False),\n",
    "        StructField(\"URL_FILE\", StringType(), nullable=False),\n",
    "        StructField(\"PAGE_NAME\", StringType(), nullable=False),\n",
    "        StructField(\"REF_URL_category\", StringType(), nullable=False),\n",
    "        StructField(\"page_load_error\", IntegerType(), nullable=False),\n",
    "        StructField(\"page_action_detail\", StringType(), nullable=False),\n",
    "        StructField(\"tip\", StringType(), nullable=False),\n",
    "        StructField(\"service_detail\", StringType(), nullable=False),\n",
    "        StructField(\"xps_info\", StringType(), nullable=False),\n",
    "        StructField(\"page_action_detail_EN\", StringType(), nullable=False),\n",
    "        StructField(\"service_detail_EN\", StringType(), nullable=False),\n",
    "        StructField(\"tip_EN\", StringType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "click_data_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .schema(click_data_schema)\n",
    "    .csv(\"./datasets/BPI2016_Clicks_NOT_Logged_In.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting dataframes to delta tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_df.write.mode(\"overwrite\").format(\"delta\").save(\"./output/users_df_delta\")\n",
    "# click_data_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "#     \"./output/click_data_df_delta\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying delta tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2024-01-17 14:01:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          0|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2024-01-17 13:54:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|     Serializable|        false|{numFiles -> 8, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2024-01-17 14:03:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          0|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2024-01-17 13:54:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|     Serializable|        false|{numFiles -> 9, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "DESCRIBE HISTORY delta.`D:\\\\development\\\\learn_spark\\\\output\\\\users_df_delta`;\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "DESCRIBE HISTORY delta.`D:\\\\development\\\\learn_spark\\\\output\\\\click_data_df_delta`;\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing EDA on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- IPID: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- VHOST: string (nullable = true)\n",
      " |-- URL_FILE: string (nullable = true)\n",
      " |-- PAGE_NAME: string (nullable = true)\n",
      " |-- REF_URL_category: string (nullable = true)\n",
      " |-- page_load_error: integer (nullable = true)\n",
      " |-- page_action_detail: string (nullable = true)\n",
      " |-- tip: string (nullable = true)\n",
      " |-- service_detail: string (nullable = true)\n",
      " |-- xps_info: string (nullable = true)\n",
      " |-- page_action_detail_EN: string (nullable = true)\n",
      " |-- service_detail_EN: string (nullable = true)\n",
      " |-- tip_EN: string (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+-------------+--------------------+------------------+----------------+---------------+------------------+----+--------------+--------------------+---------------------+-----------------+------+\n",
      "|session_id|   IPID|           timestamp|        VHOST|            URL_FILE|         PAGE_NAME|REF_URL_category|page_load_error|page_action_detail| tip|service_detail|            xps_info|page_action_detail_EN|service_detail_EN|tip_EN|\n",
      "+----------+-------+--------------------+-------------+--------------------+------------------+----------------+---------------+------------------+----+--------------+--------------------+---------------------+-----------------+------+\n",
      "|  43581744| 849196|2015-12-23 10:23:...|  www.werk.nl|  /xpsitem/ptl260281|         ptl260281|            NULL|              0|              NULL|NULL|          NULL|download_model_ar...|                 NULL|             NULL|  NULL|\n",
      "|  41683444|2110820|2015-12-15 12:30:...|  www.werk.nl|/werk_nl/werkneme...|              home|      Logged Out|              0|              NULL|NULL|          NULL|                NULL|                 NULL|             NULL|  NULL|\n",
      "|  39411578|1651304|2015-12-15 10:53:...|  www.werk.nl|/werk_nl/werkneme...|         vacatures|            NULL|              0|              NULL|NULL|          NULL|                NULL|                 NULL|             NULL|  NULL|\n",
      "|  39122892|   5121|2015-12-16 14:33:...|  www.werk.nl|//werk_nl/werknem...|       inschrijven|             UWV|              0|              NULL|NULL|          NULL|                NULL|                 NULL|             NULL|  NULL|\n",
      "|  41017721| 428116|2016-01-04 11:59:...|digid.werk.nl|/portal/page/port...|aanvragen-bijstand|            NULL|              0|              NULL|NULL|1. Uw situatie|                NULL|                 NULL|1. Your situation|  NULL|\n",
      "+----------+-------+--------------------+-------------+--------------------+------------------+----------------+---------------+------------------+----+--------------+--------------------+---------------------+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- organization_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- founded: integer (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- employee_no: integer (nullable = true)\n",
      "\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+-------------------+-----------+\n",
      "|index|organization_id|                name|             website|             country|         description|founded|           industry|employee_no|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+-------------------+-----------+\n",
      "|    1|391dAA77fea9EC1|      Daniel-Mcmahon|https://stuart-ri...|            Cambodia|Focused eco-centr...|   2013|             Sports|       1878|\n",
      "|    2|9FcCA4A23e6BcfA|Mcdowell, Tate an...|  http://jacobs.biz/|              Guyana|Front-line real-t...|   2018|     Legal Services|       9743|\n",
      "|    3|DB23330238B7B3D|Roberts, Carson a...|http://www.park.com/|              Jordan|Innovative hybrid...|   1992|        Hospitality|       7537|\n",
      "|    4|bbf18835CFbEee7|Poole, Jefferson ...|  http://hayden.com/|Cocos (Keeling) I...|Extended regional...|   1991|    Food Production|       9974|\n",
      "|    5|74ECD725ceaDfd9|Ritter, Patel and...|https://www.mason...|             Ecuador|Re-contextualized...|   2019|Computer Networking|       5050|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"./output/click_data_df_delta/\").printSchema()\n",
    "spark.read.format(\"delta\").load(\"./output/click_data_df_delta/\").show(5)\n",
    "\n",
    "spark.read.format(\"delta\").load(\"./output/users_df_delta/\").printSchema()\n",
    "spark.read.format(\"delta\").load(\"./output/users_df_delta/\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "\n",
    "1. In first dataset it is a good choice to z-order the dataset by page name so the records corresponding to the same page are co-located\n",
    "2. In the second dataset the same logic as point 1 is there for the country attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\n",
    "#     \"\"\"\n",
    "# OPTIMIZE delta.`D:\\\\development\\\\learn_spark\\\\output\\\\users_df_delta` ZORDER BY (country);\n",
    "# \"\"\"\n",
    "# ).show()\n",
    "# spark.sql(\n",
    "#     \"\"\"\n",
    "# OPTIMIZE delta.`D:\\\\development\\\\learn_spark\\\\output\\\\click_data_df_delta` ZORDER BY (PAGE_NAME);\n",
    "# \"\"\"\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                       |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                      |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2024-01-17 14:01:47.535|NULL  |NULL    |OPTIMIZE |{predicate -> [], zOrderBy -> [\"country\"]}|NULL|NULL    |NULL     |0          |SnapshotIsolation|false        |{numRemovedFiles -> 8, numRemovedBytes -> 120336344, p25FileSize -> 120941341, numDeletionVectorsRemoved -> 0, minFileSize -> 120941341, numAddedFiles -> 1, maxFileSize -> 120941341, p75FileSize -> 120941341, p50FileSize -> 120941341, numAddedBytes -> 120941341}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-17 13:54:01.032|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}    |NULL|NULL    |NULL     |NULL       |Serializable     |false        |{numFiles -> 8, numOutputRows -> 2000000, numOutputBytes -> 120336344}                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                         |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                      |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2024-01-17 14:03:49.142|NULL  |NULL    |OPTIMIZE |{predicate -> [], zOrderBy -> [\"PAGE_NAME\"]}|NULL|NULL    |NULL     |0          |SnapshotIsolation|false        |{numRemovedFiles -> 9, numRemovedBytes -> 154394588, p25FileSize -> 157316493, numDeletionVectorsRemoved -> 0, minFileSize -> 157316493, numAddedFiles -> 1, maxFileSize -> 157316493, p75FileSize -> 157316493, p50FileSize -> 157316493, numAddedBytes -> 157316493}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-17 13:54:37.775|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}      |NULL|NULL    |NULL     |NULL       |Serializable     |false        |{numFiles -> 9, numOutputRows -> 9329418, numOutputBytes -> 154394588}                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "DESCRIBE HISTORY delta.`D:\\\\development\\\\learn_spark\\\\output\\\\users_df_delta`;\n",
    "\"\"\"\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "DESCRIBE HISTORY delta.`D:\\\\development\\\\learn_spark\\\\output\\\\click_data_df_delta`;\n",
    "\"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading delta tables in spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_delta_table_v1 = (\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./output/users_df_delta/\")\n",
    ")\n",
    "user_delta_table_v2 = (\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"./output/users_df_delta/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_delta_table_v2.createOrReplaceTempView(\"user_delta_table_v2_tv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " index           | 1                             \n",
      " organization_id | 391dAA77fea9EC1               \n",
      " name            | Daniel-Mcmahon                \n",
      " website         | https://stuart-rios.biz/      \n",
      " country         | Cambodia                      \n",
      " description     | Focused eco-centric help-desk \n",
      " founded         | 2013                          \n",
      " industry        | Sports                        \n",
      " employee_no     | 1878                          \n",
      "-RECORD 1----------------------------------------\n",
      " index           | 2                             \n",
      " organization_id | 9FcCA4A23e6BcfA               \n",
      " name            | Mcdowell, Tate and Murray     \n",
      " website         | http://jacobs.biz/            \n",
      " country         | Guyana                        \n",
      " description     | Front-line real-time portal   \n",
      " founded         | 2018                          \n",
      " industry        | Legal Services                \n",
      " employee_no     | 9743                          \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_delta_table_v2.show(2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfrom Group By and aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+--------------+\n",
      "|employee_no|employee_count|        country_list|contains_india|\n",
      "+-----------+--------------+--------------------+--------------+\n",
      "|       4846|           261|[Samoa, Malta, Ja...|          true|\n",
      "|       2384|           251|[Slovakia (Slovak...|          true|\n",
      "|       3390|           250|[Rwanda, Uruguay,...|         false|\n",
      "|       1589|           249|[Iran, Puerto Ric...|          true|\n",
      "|       1961|           247|[Macedonia, Malay...|         false|\n",
      "+-----------+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT\n",
    "#     name,\n",
    "#     count(distinct country) as distinct_country_count,\n",
    "#     count(distinct employee_no) as distinct_no_of_employees\n",
    "#     from user_delta_table_v2_tv\n",
    "#     group by name\n",
    "#     order by distinct_no_of_employees desc;\n",
    "#     \"\"\"\n",
    "# ).show(5)\n",
    "\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "    \n",
    "    employee_no,\n",
    "    count(employee_no) as employee_count,\n",
    "    array_distinct(array_agg(country)) as country_list,\n",
    "    array_contains(array_agg(country), 'India') as contains_india\n",
    "\n",
    "    from user_delta_table_v2_tv \n",
    "    group by employee_no\n",
    "    order by employee_count desc;\n",
    "    \"\"\"\n",
    ").show(5)\n",
    "\n",
    "\n",
    "# spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT * from user_delta_table_v2_tv where name like '%Bradley PLC%';\n",
    "#     \"\"\"\n",
    "# ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---------------------+---------------+-----------------+\n",
      "|employee_no|employee_no_count|distinct_country_list|is_india_a_part|average_employees|\n",
      "+-----------+-----------------+---------------------+---------------+-----------------+\n",
      "|       4846|              261| [Samoa, Malta, Ja...|           true|           4846.0|\n",
      "|       2384|              251| [Slovakia (Slovak...|           true|           2384.0|\n",
      "|       3390|              250| [Rwanda, Uruguay,...|          false|           3390.0|\n",
      "|       1589|              249| [Iran, Puerto Ric...|           true|           1589.0|\n",
      "|       7828|              247| [Saint Vincent an...|           true|           7828.0|\n",
      "+-----------+-----------------+---------------------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_delta_table_v2.groupBy(\"employee_no\").agg(\n",
    "    count(user_delta_table_v2.employee_no).alias(\"employee_no_count\"),\n",
    "    array_distinct(array_agg(user_delta_table_v2.country)).alias(\n",
    "        \"distinct_country_list\"\n",
    "    ),\n",
    "    array_contains(array_agg(user_delta_table_v2.country), \"India\").alias(\n",
    "        \"is_india_a_part\"\n",
    "    ),\n",
    "    avg(user_delta_table_v2.employee_no).alias(\"average_employees\"),\n",
    ").sort(desc(\"employee_no_count\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_salaries_df = spark.read.options(header=True, inferSchema=True).csv(\n",
    "    \"./datasets/ds_salaries.csv\"\n",
    ")\n",
    "ds_salaries_df.createOrReplaceTempView(\"ds_salaries_tv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available functions\n",
    "\n",
    "- Simple aggregation functions: avg, sum, min, max, count\n",
    "- Row-wise ordering and ranking functions: row_number, rank, dense_rank, percent_rank, ntile (divides data into n buckets)\n",
    "- Creating lagged columns: lag, lead\n",
    "- Combining Windows and Calling Functions: over\n",
    "- Analytic functions: cume_dist, first_value, last_value, nth_value\n",
    "- Aggregate functions: collect_list, collect_set, corr, covar_pop, covar_samp, stddev, stddev_pop, stddev_samp, variance, var_pop, var_samp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+------+----+----------+------------------+-----+------+-----+------------------+------+------+------+-------------------+\n",
      "|_c0|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|row_no|rank|dense_rank|      percent_rank|ntile|  lead|  lag|               avg|   min|   max|   sum|            cumdist|\n",
      "+---+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+------+----+----------+------------------+-----+------+-----+------------------+------+------+------+-------------------+\n",
      "| 77|     2021|              MI|             PT|3D Computer Visio...|400000|            INR|         5409|                IN|          50|              IN|           M|     1|   1|         1|               0.0|    1|   404|  404|          400000.0|400000|400000|400000|                1.0|\n",
      "| 96|     2021|              EN|             PT|        AI Scientist| 12000|            USD|        12000|                BR|         100|              US|           S|     1|   1|         1|               0.0|    1| 12000|  404|           12000.0| 12000| 12000| 24000| 0.2857142857142857|\n",
      "|113|     2021|              EN|             PT|        AI Scientist| 12000|            USD|        12000|                PK|         100|              US|           M|     2|   1|         1|               0.0|    1| 55000|12000|           12000.0| 12000| 12000| 24000| 0.2857142857142857|\n",
      "|277|     2021|              SE|             FT|        AI Scientist| 55000|            USD|        55000|                ES|         100|              ES|           L|     3|   3|         2|0.3333333333333333|    1|120000|12000|26333.333333333332| 12000| 55000| 79000|0.42857142857142855|\n",
      "|391|     2022|              MI|             FT|        AI Scientist|120000|            USD|       120000|                US|           0|              US|           M|     4|   4|         3|               0.5|    2|200000|55000|           49750.0| 12000|120000|199000| 0.5714285714285714|\n",
      "+---+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+------+----+----------+------------------+-----+------+-----+------------------+------+------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "select\n",
    "\n",
    "tab.*,\n",
    "\n",
    "\n",
    "ROW_NUMBER() OVER(PARTITION BY job_title ORDER BY salary) as row_no,\n",
    "RANK() OVER(PARTITION BY job_title ORDER BY salary) as rank,\n",
    "dense_rank() OVER(PARTITION BY job_title ORDER BY salary) as dense_rank,\n",
    "PERCENT_RANK() OVER(PARTITION BY job_title ORDER BY salary) as percent_rank,\n",
    "NTILE(3) OVER(PARTITION BY job_title ORDER BY salary) as ntile,\n",
    "lead(salary, 1, 404) OVER(PARTITION BY job_title ORDER BY salary) as lead,\n",
    "lag(salary, 1, 404) OVER(PARTITION BY job_title ORDER BY salary) as lag,\n",
    "\n",
    "avg(salary) OVER(PARTITION BY job_title ORDER BY salary) as avg,\n",
    "min(salary) OVER(PARTITION BY job_title ORDER BY salary) as min,\n",
    "max(salary) OVER(PARTITION BY job_title ORDER BY salary) as max,\n",
    "sum(salary) OVER(PARTITION BY job_title ORDER BY salary) as sum,\n",
    "cume_dist() OVER(PARTITION BY job_title ORDER BY salary) as cumdist\n",
    "\n",
    "\n",
    "from ds_salaries_tv as tab;\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+----------+----+----------+------------------+------+--------+--------+--------+------+-------------------+\n",
      "|_c0|work_year|experience_level|employment_type|           job_title|  salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|row_number|rank|dense_rank|               avg|   min|     max|     sum|    lead|   lag|          cume_dist|\n",
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+----------+----+----------+------------------+------+--------+--------+--------+------+-------------------+\n",
      "| 77|     2021|              MI|             PT|3D Computer Visio...|  400000|            INR|         5409|                IN|          50|              IN|           M|         1|   1|         1|          400000.0|400000|  400000|  400000|    NULL|  NULL|                1.0|\n",
      "| 96|     2021|              EN|             PT|        AI Scientist|   12000|            USD|        12000|                BR|         100|              US|           S|         1|   1|         1|           12000.0| 12000|   12000|   24000|   55000|  NULL| 0.2857142857142857|\n",
      "|113|     2021|              EN|             PT|        AI Scientist|   12000|            USD|        12000|                PK|         100|              US|           M|         2|   1|         1|           12000.0| 12000|   12000|   24000|  120000|  NULL| 0.2857142857142857|\n",
      "|277|     2021|              SE|             FT|        AI Scientist|   55000|            USD|        55000|                ES|         100|              ES|           L|         3|   3|         2|26333.333333333332| 12000|   55000|   79000|  200000| 12000|0.42857142857142855|\n",
      "|391|     2022|              MI|             FT|        AI Scientist|  120000|            USD|       120000|                US|           0|              US|           M|         4|   4|         3|           49750.0| 12000|  120000|  199000|  300000| 12000| 0.5714285714285714|\n",
      "|606|     2022|              MI|             FT|        AI Scientist|  200000|            USD|       200000|                IN|         100|              US|           L|         5|   5|         4|           79800.0| 12000|  200000|  399000| 1335000| 55000| 0.7142857142857143|\n",
      "| 52|     2020|              EN|             FT|        AI Scientist|  300000|            DKK|        45896|                DK|          50|              DK|           S|         6|   6|         5|          116500.0| 12000|  300000|  699000|    NULL|120000| 0.8571428571428571|\n",
      "|244|     2021|              EN|             FT|        AI Scientist| 1335000|            INR|        18053|                IN|         100|              AS|           S|         7|   7|         6| 290571.4285714286| 12000| 1335000| 2034000|    NULL|200000|                1.0|\n",
      "|368|     2022|              EX|             FT|  Analytics Engineer|  135000|            USD|       135000|                US|         100|              US|           M|         1|   1|         1|          135000.0|135000|  135000|  135000|  184700|  NULL|               0.25|\n",
      "|344|     2022|              EX|             FT|  Analytics Engineer|  175000|            USD|       175000|                US|         100|              US|           M|         2|   2|         2|          155000.0|135000|  175000|  310000|  205300|  NULL|                0.5|\n",
      "|561|     2022|              SE|             FT|  Analytics Engineer|  184700|            USD|       184700|                US|           0|              US|           M|         3|   3|         3|          164900.0|135000|  184700|  494700|    NULL|135000|               0.75|\n",
      "|560|     2022|              SE|             FT|  Analytics Engineer|  205300|            USD|       205300|                US|           0|              US|           M|         4|   4|         4|          175000.0|135000|  205300|  700000|    NULL|175000|                1.0|\n",
      "| 82|     2021|              MI|             FT|Applied Data Scie...|   68000|            CAD|        54238|                GB|          50|              CA|           L|         1|   1|         1|           68000.0| 68000|   68000|   68000|  157000|  NULL|                0.2|\n",
      "|123|     2021|              EN|             FT|Applied Data Scie...|   80000|            GBP|       110037|                GB|           0|              GB|           L|         2|   2|         2|           74000.0| 68000|   80000|  148000|  177000|  NULL|                0.4|\n",
      "|509|     2022|              MI|             FT|Applied Data Scie...|  157000|            USD|       157000|                US|         100|              US|           L|         3|   3|         3|101666.66666666667| 68000|  157000|  305000|  380000| 68000|                0.6|\n",
      "|525|     2022|              SE|             FT|Applied Data Scie...|  177000|            USD|       177000|                US|         100|              US|           L|         4|   4|         4|          120500.0| 68000|  177000|  482000|    NULL| 80000|                0.8|\n",
      "|519|     2022|              SE|             FT|Applied Data Scie...|  380000|            USD|       380000|                US|         100|              US|           L|         5|   5|         5|          172400.0| 68000|  380000|  862000|    NULL|157000|                1.0|\n",
      "|489|     2022|              EN|             CT|Applied Machine L...|   29000|            EUR|        31875|                TN|         100|              CZ|           M|         1|   1|         1|           29000.0| 29000|   29000|   29000|   75000|  NULL|               0.25|\n",
      "|132|     2021|              MI|             FT|Applied Machine L...|   38400|            USD|        38400|                VN|         100|              US|           M|         2|   2|         2|           33700.0| 29000|   38400|   67400|  423000|  NULL|                0.5|\n",
      "|506|     2022|              MI|             FT|Applied Machine L...|   75000|            USD|        75000|                BO|         100|              US|           L|         3|   3|         3|47466.666666666664| 29000|   75000|  142400|    NULL| 29000|               0.75|\n",
      "|157|     2021|              MI|             FT|Applied Machine L...|  423000|            USD|       423000|                US|          50|              US|           L|         4|   4|         4|          141350.0| 29000|  423000|  565400|    NULL| 38400|                1.0|\n",
      "|196|     2021|              EN|             FT|     BI Data Analyst|    9272|            USD|         9272|                KE|         100|              KE|           S|         1|   1|         1|            9272.0|  9272|    9272|    9272|   98000|  NULL|0.16666666666666666|\n",
      "|168|     2021|              EN|             FT|     BI Data Analyst|   55000|            USD|        55000|                US|          50|              US|           S|         2|   2|         2|           32136.0|  9272|   55000|   64272|  100000|  NULL| 0.3333333333333333|\n",
      "| 23|     2020|              MI|             FT|     BI Data Analyst|   98000|            USD|        98000|                US|           0|              US|           M|         3|   3|         3|54090.666666666664|  9272|   98000|  162272|  150000|  9272|                0.5|\n",
      "| 76|     2021|              MI|             FT|     BI Data Analyst|  100000|            USD|       100000|                US|         100|              US|           M|         4|   4|         4|           65568.0|  9272|  100000|  262272|11000000| 55000| 0.6666666666666666|\n",
      "| 73|     2021|              EX|             FT|     BI Data Analyst|  150000|            USD|       150000|                IN|         100|              US|           L|         5|   5|         5|           82454.4|  9272|  150000|  412272|    NULL| 98000| 0.8333333333333334|\n",
      "|102|     2021|              MI|             FT|     BI Data Analyst|11000000|            HUF|        36259|                HU|          50|              US|           L|         6|   6|         6|1902045.3333333333|  9272|11000000|11412272|    NULL|100000|                1.0|\n",
      "|255|     2021|              SE|             FT|  Big Data Architect|  125000|            CAD|        99703|                CA|          50|              CA|           M|         1|   1|         1|          125000.0|125000|  125000|  125000|    NULL|  NULL|                1.0|\n",
      "|192|     2021|              MI|             FT|   Big Data Engineer|   18000|            USD|        18000|                MD|           0|              MD|           S|         1|   1|         1|           18000.0| 18000|   18000|   18000|   70000|  NULL|              0.125|\n",
      "|120|     2021|              MI|             FT|   Big Data Engineer|   60000|            USD|        60000|                ES|          50|              RO|           M|         2|   2|         2|           39000.0| 18000|   60000|   78000|   85000|  NULL|               0.25|\n",
      "| 31|     2020|              EN|             FT|   Big Data Engineer|   70000|            USD|        70000|                US|         100|              US|           L|         3|   3|         3|49333.333333333336| 18000|   70000|  148000|  100000| 18000|              0.375|\n",
      "|  2|     2020|              SE|             FT|   Big Data Engineer|   85000|            GBP|       109024|                GB|          50|              GB|           M|         4|   4|         4|           58250.0| 18000|   85000|  233000|  435000| 60000|                0.5|\n",
      "| 17|     2020|              SE|             FT|   Big Data Engineer|  100000|            EUR|       114047|                PL|         100|              GB|           S|         5|   5|         5|           66600.0| 18000|  100000|  333000| 1200000| 70000|              0.625|\n",
      "|213|     2021|              EN|             FT|   Big Data Engineer|  435000|            INR|         5882|                IN|           0|              CH|           L|         6|   6|         6|          128000.0| 18000|  435000|  768000| 1672000| 85000|               0.75|\n",
      "|230|     2021|              EN|             FT|   Big Data Engineer| 1200000|            INR|        16228|                IN|         100|              IN|           L|         7|   7|         7|281142.85714285716| 18000| 1200000| 1968000|    NULL|100000|              0.875|\n",
      "|180|     2021|              MI|             FT|   Big Data Engineer| 1672000|            INR|        22611|                IN|           0|              IN|           L|         8|   8|         8|          455000.0| 18000| 1672000| 3640000|    NULL|435000|                1.0|\n",
      "|279|     2021|              EN|             FT|Business Data Ana...|   50000|            EUR|        59102|                LU|         100|              LU|           L|         1|   1|         1|           50000.0| 50000|   50000|   50000|  100000|  NULL|                0.2|\n",
      "|511|     2022|              MI|             FT|Business Data Ana...|   90000|            CAD|        70912|                CA|          50|              CA|           L|         2|   2|         2|           70000.0| 50000|   90000|  140000|  135000|  NULL|                0.4|\n",
      "| 28|     2020|              EN|             CT|Business Data Ana...|  100000|            USD|       100000|                US|         100|              US|           L|         3|   3|         3|           80000.0| 50000|  100000|  240000| 1400000| 50000|                0.6|\n",
      "|  8|     2020|              MI|             FT|Business Data Ana...|  135000|            USD|       135000|                US|         100|              US|           L|         4|   4|         4|           93750.0| 50000|  135000|  375000|    NULL| 90000|                0.8|\n",
      "|458|     2022|              MI|             FT|Business Data Ana...| 1400000|            INR|        18442|                IN|         100|              IN|           M|         5|   5|         5|          355000.0| 50000| 1400000| 1775000|    NULL|100000|                1.0|\n",
      "| 95|     2021|              MI|             FT| Cloud Data Engineer|  120000|            SGD|        89294|                SG|          50|              SG|           L|         1|   1|         1|          120000.0|120000|  120000|  120000|    NULL|  NULL|                0.5|\n",
      "|149|     2021|              SE|             FT| Cloud Data Engineer|  160000|            USD|       160000|                BR|         100|              US|           S|         2|   2|         2|          140000.0|120000|  160000|  280000|    NULL|  NULL|                1.0|\n",
      "|521|     2022|              EN|             FT|Computer Vision E...|   10000|            USD|        10000|                PT|         100|              LU|           M|         1|   1|         1|           10000.0| 10000|   10000|   10000|   60000|  NULL|0.16666666666666666|\n",
      "|133|     2021|              SE|             FT|Computer Vision E...|   24000|            USD|        24000|                BR|         100|              BR|           M|         2|   2|         2|           17000.0| 10000|   24000|   34000|  102000|  NULL| 0.3333333333333333|\n",
      "| 54|     2020|              SE|             FL|Computer Vision E...|   60000|            USD|        60000|                RU|         100|              US|           S|         3|   3|         3|31333.333333333332| 10000|   60000|   94000|  125000| 10000|                0.5|\n",
      "|271|     2021|              SE|             FT|Computer Vision E...|  102000|            BRL|        18907|                BR|           0|              BR|           M|         4|   4|         4|           49000.0| 10000|  102000|  196000|  180000| 24000| 0.6666666666666666|\n",
      "|454|     2022|              EN|             FT|Computer Vision E...|  125000|            USD|       125000|                US|           0|              US|           M|         5|   5|         5|           64200.0| 10000|  125000|  321000|    NULL| 60000| 0.8333333333333334|\n",
      "|216|     2021|              EN|             PT|Computer Vision E...|  180000|            DKK|        28609|                DK|          50|              DK|           S|         6|   6|         6|           83500.0| 10000|  180000|  501000|    NULL|102000|                1.0|\n",
      "| 98|     2021|              EN|             FT|Computer Vision S...|   70000|            USD|        70000|                US|         100|              US|           M|         1|   1|         1|           70000.0| 70000|   70000|   70000|  150000|  NULL| 0.3333333333333333|\n",
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+----------+----+----------+------------------+------+--------+--------+--------+------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"job_title\").orderBy(\"salary\")\n",
    "\n",
    "\n",
    "ds_salaries_df.withColumn(\"row_number\", row_number().over(windowSpec)).withColumn(\n",
    "    \"rank\", rank().over(windowSpec)\n",
    ").withColumn(\"dense_rank\", dense_rank().over(windowSpec)).withColumn(\n",
    "    \"avg\", avg(col(\"salary\")).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"min\", min(col(\"salary\")).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"max\", max(col(\"salary\")).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"sum\", sum(col(\"salary\")).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"lead\", lead(\"salary\", 2).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"lag\", lag(\"salary\", 2).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"cume_dist\", cume_dist().over(windowSpec)\n",
    ").show(\n",
    "    50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Data Warehousing Technologies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolyBase is a technology that enables the integration and querying of both relational and non-relational data across on-premises and cloud environments in a seamless manner. It was originally developed by Microsoft and is part of their data platform offerings, including SQL Server and Azure Synapse Analytics (formerly SQL Data Warehouse).\n",
    "\n",
    "Key features and aspects of PolyBase include:\n",
    "\n",
    "1. **Unified Query Processing:**\n",
    "   PolyBase allows users to query data stored in relational databases, data warehouses, and non-relational data sources using a standard SQL interface. This unified query processing enables the integration of diverse data types.\n",
    "\n",
    "2. **Data Virtualization:**\n",
    "   With PolyBase, you can create external tables that reference data stored in external sources without physically moving the data. This data virtualization feature makes it possible to query and join data from different sources as if it were stored in a single location.\n",
    "\n",
    "3. **Parallel Processing:**\n",
    "   PolyBase leverages parallel processing capabilities to enhance performance. It allows for the parallel execution of queries across distributed data sources, making it well-suited for large-scale analytics and data warehousing.\n",
    "\n",
    "4. **Integration with Hadoop:**\n",
    "   Originally, PolyBase was designed to integrate with Hadoop-based systems. It can read and write data directly to Hadoop Distributed File System (HDFS), providing a bridge between SQL-based systems and the Hadoop ecosystem.\n",
    "\n",
    "5. **Support for Azure Blob Storage and Azure Data Lake Storage:**\n",
    "   In addition to Hadoop integration, PolyBase also supports Azure Blob Storage and Azure Data Lake Storage, allowing users to seamlessly query and analyze data stored in Azure cloud storage.\n",
    "\n",
    "6. **Azure Synapse Analytics Integration:**\n",
    "   PolyBase is a key component of Azure Synapse Analytics, where it is used to integrate data stored in data lakes, data warehouses, and other sources. It enables users to perform analytics and reporting across various data platforms within the Azure ecosystem.\n",
    "\n",
    "7. **External Tables and External Data Sources:**\n",
    "   Users define external tables in SQL Server or Azure Synapse Analytics that reference data stored externally. External data sources are used to establish a connection to external data storage locations.\n",
    "\n",
    "8. **Security:**\n",
    "   PolyBase includes security features to ensure that sensitive data is protected. It supports authentication mechanisms and encryption for data in transit.\n",
    "\n",
    "PolyBase is particularly beneficial in scenarios where organizations have a mix of structured and unstructured data stored in different locations, and they need a unified approach to query and analyze that data. It simplifies the process of working with data across diverse data platforms, providing a more integrated and efficient data analytics experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Microsoft Azure, hierarchy often refers to the organization of resources within Azure Active Directory (Azure AD) or the structure of resources within Azure Resource Manager (ARM) for managing and organizing resources. Let's explore both aspects:\n",
    "\n",
    "### 1. **Azure Active Directory Hierarchy:**\n",
    "\n",
    "Azure AD is Microsoft's cloud-based identity and access management service. In the context of Azure AD, hierarchy can refer to the structure of directories and the relationships between various entities. Key components related to hierarchy in Azure AD include:\n",
    "\n",
    "- **Directories (Tenants):** Azure AD supports multiple directories, also known as tenants. Each directory is an isolated instance with its own users, groups, and applications. The hierarchy here involves the relationships between different directories.\n",
    "\n",
    "- **Users and Groups:** Within each directory, you have users and groups. Users can be organized into groups to simplify access management. Groups can also be nested to form a hierarchical structure.\n",
    "\n",
    "- **Roles:** Azure AD provides built-in roles and custom roles that grant users specific permissions. Roles can be assigned at different levels, such as the directory level, subscription level, or resource group level, contributing to the hierarchy of access control.\n",
    "\n",
    "### 2. **Azure Resource Manager (ARM) Hierarchy:**\n",
    "\n",
    "Azure Resource Manager is the deployment and management service in Azure that allows you to work with resources as a group. In ARM, hierarchy refers to the structure of resources within a subscription and resource groups. Key concepts related to hierarchy in ARM include:\n",
    "\n",
    "- **Subscription:** The top-level container for resources in Azure. Subscriptions are associated with billing and access control boundaries.\n",
    "\n",
    "- **Resource Groups:** Resources are organized into resource groups, which are logical containers that help manage and organize resources. Resource groups can be considered as the next level of hierarchy under a subscription.\n",
    "\n",
    "- **Resources:** Azure resources, such as virtual machines, storage accounts, databases, etc., are organized within resource groups. Each resource type has its own set of properties and configurations.\n",
    "\n",
    "- **Management Groups:** Azure management groups are a way to organize subscriptions within an organization. They provide a level of hierarchy above subscriptions and allow for centralized policy enforcement and governance.\n",
    "\n",
    "### Example (ARM Hierarchy):\n",
    "\n",
    "- Subscription (e.g., \"MyAzureSubscription\")\n",
    "  - Resource Group (e.g., \"RG-WebApp\")\n",
    "    - App Service (e.g., \"MyWebApp\")\n",
    "    - SQL Database (e.g., \"MyDatabase\")\n",
    "  - Resource Group (e.g., \"RG-Storage\")\n",
    "    - Storage Account (e.g., \"MyStorageAccount\")\n",
    "\n",
    "The hierarchy in ARM allows for logical grouping of resources, making it easier to manage and organize resources within a subscription.\n",
    "\n",
    "In summary, hierarchy in Microsoft Azure encompasses the organization and relationships between resources in both Azure Active Directory and Azure Resource Manager. Understanding these hierarchies is crucial for effective resource management, access control, and governance in the Azure cloud environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                                   | Azure Data Factory                                                                                                                     | Azure Synapse Analytics                                                                                                  |\n",
    "| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Purpose**                               | Data integration and orchestration service for ETL (Extract, Transform, Load) workflows.                                               | Integrated analytics service that brings big data and data warehousing together.                                         |\n",
    "| **Primary Use Case**                      | Moving and transforming data across various data stores, both on-premises and in the cloud.                                            | Data warehousing, big data analytics, and data integration within a unified environment.                                 |\n",
    "| **Data Processing Engine**                | Uses Azure Data Factory Managed Clusters for ETL processing.                                                                           | Combines on-demand and provisioned resources with MPP (Massively Parallel Processing) architecture.                      |\n",
    "| **Data Integration Capabilities**         | Supports diverse data sources and destinations, including cloud and on-premises databases, flat files, and more.                       | Integrates data across various sources, including data lakes and data warehouses.                                        |\n",
    "| **Transformation Capabilities**           | Provides data wrangling and transformation using Mapping Data Flow.                                                                    | Offers data transformations through PolyBase, T-SQL, and various built-in functions.                                     |\n",
    "| **Scale**                                 | Scales vertically by adjusting the Data Factory configuration.                                                                         | Scales horizontally by adding more Data Warehouse Units (DWUs) in Synapse Analytics.                                     |\n",
    "| **Query Language**                        | Uses Azure Data Factory Data Flow for transformations, and supports various languages for data movement and transformation activities. | Utilizes T-SQL (Transact-SQL) for querying data in Synapse SQL Pools.                                                    |\n",
    "| **Integration with Other Azure Services** | Integrates with various Azure services, including Azure Storage, Azure SQL Database, Azure Databricks, and more.                       | Deep integration with Azure services, such as Azure Machine Learning, Power BI, and Azure Purview.                       |\n",
    "| **Data Security**                         | Provides security features such as Managed Virtual Network, Data Encryption, and Managed Private Endpoints.                            | Implements security features like Transparent Data Encryption (TDE), Row-level Security (RLS), and dynamic data masking. |\n",
    "| **Cost Model**                            | Pay-as-you-go pricing based on the number of data movement and data transformation activities.                                         | Consumption-based pricing with separate costs for data storage and data processing.                                      |\n",
    "| **Management and Monitoring**             | Offers Azure Monitor integration for monitoring and managing pipelines.                                                                | Includes Azure Monitor for performance monitoring and Azure Security Center for security monitoring.                     |\n",
    "| **Scalability**                           | Scalable, but scaling requires adjusting the configuration of the Data Factory.                                                        | Highly scalable using on-demand resources and the ability to adjust DWUs based on workload demands.                      |\n",
    "| **Real-time Analytics**                   | Limited support for real-time analytics.                                                                                               | Supports real-time analytics through features like streaming ingest and PolyBase external tables.                        |\n",
    "| **Hybrid Cloud Support**                  | Supports hybrid cloud scenarios for connecting on-premises data sources.                                                               | Designed for cloud-native analytics but can connect to on-premises data sources using PolyBase.                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below is a detailed comparison between transactional tables and dimensional tables presented in a tabular format:\n",
    "\n",
    "| Feature                 | Transactional Tables                                        | Dimensional Tables                                              |\n",
    "| ----------------------- | ----------------------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **Purpose**             | Capture and store detailed, real-time transactional data.   | Organize data for analysis and reporting in a data warehouse.   |\n",
    "| **Granularity**         | Fine-grained with individual transactions.                  | Coarser-grained with aggregated or summarized data.             |\n",
    "| **Data Volume**         | Typically large due to detailed transactional records.      | Relatively smaller compared to transactional tables.            |\n",
    "| **Schema Design**       | Often normalized to reduce redundancy.                      | Typically denormalized for simplicity and performance.          |\n",
    "| **Structure**           | Typically contain many attributes and relationships.        | Usually contain fewer attributes, focused on descriptive data.  |\n",
    "| **Updates and Inserts** | Frequent updates and inserts for real-time data.            | Relatively stable with occasional batch updates.                |\n",
    "| **Indexes**             | Often have indexes to optimize transactional queries.       | May have indexes, but the focus is on query performance.        |\n",
    "| **Query Complexity**    | Complex queries to retrieve specific transactional details. | Simplified queries for reporting and analysis.                  |\n",
    "| **Historical Data**     | May keep historical data for audit or compliance purposes.  | May keep historical data for tracking changes over time.        |\n",
    "| **Join Operations**     | Frequently involve multiple joins for detailed analysis.    | Less reliance on joins; more focus on star or snowflake schema. |\n",
    "| **Commonly Used in**    | OLTP (Online Transaction Processing) systems.               | OLAP (Online Analytical Processing) systems.                    |\n",
    "| **Examples**            | Order details, transaction records, customer interactions.  | Customer dimension, product dimension, time dimension.          |\n",
    "\n",
    "This table outlines the key differences between transactional tables, which are optimized for capturing and processing individual transactions in real-time, and dimensional tables, which are designed for analytical reporting and data analysis in a data warehouse environment. The choice between the two types depends on the specific needs of the application or analytical workload.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Schema and Snowflake Schema\n",
    "\n",
    "## What is star Schema ?\n",
    "\n",
    "A `star schema` is a multi-dimensional data model used to organize data in a database so that it is easy to understand and analyze. Star schemas can be applied to data warehouses, databases, data marts, and other tools. The star schema design is optimized for querying large data sets.\n",
    "\n",
    "Introduced by Ralph Kimball in the 1990s, star schemas are efficient at storing data, maintaining history, and updating data by reducing the duplication of repetitive business definitions, making it fast to aggregate and filter data in the data warehouse.\n",
    "\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\">\n",
    "\n",
    "### Fact tables and dimension tables\n",
    "\n",
    "A star schema is used to denormalize business data into dimensions (like time and product) and facts (like transactions in amounts and quantities).\n",
    "\n",
    "A star schema has a single fact table in the center, containing business \"facts\" (like transaction amounts and quantities). The fact table connects to multiple other dimension tables along \"dimensions\" like time, or product. Star schemas enable users to slice and dice the data however they see fit, typically by joining two or more fact tables and dimension tables together.\n",
    "\n",
    "### Denormalized data\n",
    "\n",
    "Star schemas denormalize the data, which means adding redundant columns to some dimension tables to make querying and working with the data faster and easier. The purpose is to trade some redundancy (duplication of data) in the data model for increased query speed, by avoiding computationally expensive join operations.\n",
    "\n",
    "In this model, the fact table is normalized but the dimensions tables are not. That is, data from the fact table exists only on the fact table, but dimensional tables may hold redundant data.\n",
    "\n",
    "### Benefits of star schemas\n",
    "\n",
    "1. Fact/dimensional models like star schemas are simple to understand and implement, and make it easy for end users to find the data they need. They can be applied to data marts and other data resources.\n",
    "2. Great for simple queries because of their reduced dependency on joins when accessing the data, as compared to normalized models like snowflake schemas.\n",
    "3. Adapt well to fit OLAP models.\n",
    "4. Improved query performance as compared to normalized data, because star schemas attempt to avoid computationally expensive joins.\n",
    "\n",
    "### How does a star schema differ from 3NF (Third Normal Form)?\n",
    "\n",
    "3NF, or Third Normal Form, is a method of reducing data-redundancy through normalization. It is a common standard for databases that are considered fully normalized. It typically has more tables than a star schema due to data normalization. On the flip-side, queries tend to be more complex due to the increased number of joins between large tables.\n",
    "\n",
    "## What is a snowflake schema?\n",
    "\n",
    "A snowflake schema is a multi-dimensional data model that is an extension of a star schema, where dimension tables are broken down into subdimensions. Snowflake schemas are commonly used for business intelligence and reporting in OLAP data warehouses, data marts, and relational databases.\n",
    "\n",
    "In a snowflake schema, engineers break down individual dimension tables into logical subdimensions. This makes the data model more complex, but it can be easier for analysts to work with, especially for certain data types.\n",
    "\n",
    "It's called a snowflake schema because its entity-relationship diagram (ERD) looks like a snowflake, as seen below.\n",
    "\n",
    "<img src = \"https://cms.databricks.com/sites/default/files/inline-images/snowflake-schema-120723_0.png\">\n",
    "\n",
    "### Snowflake schemas vs. star schemas\n",
    "\n",
    "Like star schemas, snowflake schemas have a central fact table which is connected to multiple dimension tables via foreign keys. However, the main difference is that they are more normalized than star schemas.\n",
    "\n",
    "Snowflake schemas offer more storage efficiency, due to their tighter adherence to high normalization standards, but query performance is not as good as with more denormalized data models. Denormalized data models like star schemas have more data redundancy (duplication of data), which makes query performance faster at the cost of duplicated data.\n",
    "\n",
    "### Benefits of snowflake schemas\n",
    "\n",
    "1. Fast data retrieval\n",
    "2. Enforces data quality\n",
    "3. Simple, common data model for data warehousing\n",
    "\n",
    "### Drawbacks of snowflake schemas\n",
    "\n",
    "1. Lots of overhead upon initial setup\n",
    "2. Rigid data model\n",
    "3. High maintenance costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below is a tabular comparison between Snowflake Schema and Star Schema:\n",
    "\n",
    "| Feature                      | Snowflake Schema                                                             | Star Schema                                                                  |\n",
    "| ---------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Normalization**            | More normalized, with dimension tables further split into sub-dimensions.    | Denormalized, with fewer levels of hierarchy and less normalization.         |\n",
    "| **Table Structure**          | Complex structure with multiple levels of related tables.                    | Simplified structure with a central fact table and dimension tables.         |\n",
    "| **Join Complexity**          | Higher join complexity due to more tables and relationships.                 | Lower join complexity, typically involving fewer tables.                     |\n",
    "| **Query Performance**        | May result in slightly slower query performance due to more joins.           | Generally faster query performance, especially for certain types of queries. |\n",
    "| **Storage Space**            | May require more storage space due to normalization.                         | Often requires less storage space due to denormalization.                    |\n",
    "| **Ease of Maintenance**      | May be more complex to maintain due to the multiple levels of relationships. | Simpler to maintain, as there are fewer tables and relationships.            |\n",
    "| **Flexibility in Reporting** | Offers flexibility in reporting due to detailed sub-dimensions.              | Provides simplicity for reporting, especially for common queries.            |\n",
    "| **Scalability**              | May be less scalable due to more tables and joins.                           | Generally more scalable, especially for read-heavy workloads.                |\n",
    "| **Common Use Case**          | Well-suited for complex business scenarios requiring detailed analysis.      | Commonly used for data warehousing and business intelligence applications.   |\n",
    "| **Examples**                 | Healthcare systems, financial systems with complex hierarchies.              | Retail sales reporting, customer relationship management (CRM).              |\n",
    "\n",
    "This table provides a high-level comparison between the Snowflake Schema and Star Schema, two common schema designs used in data warehousing. The choice between them depends on the specific requirements of the data warehouse and the nature of the analytical queries that need to be supported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Normalization forms\n",
    "\n",
    "Normalization is a process in database design that helps organize data in a relational database to reduce redundancy, improve data integrity, and enhance data management efficiency. The normalization process is defined by a series of normal forms, each addressing specific issues related to data organization. The most commonly discussed normalization forms are the first normal form (1NF), second normal form (2NF), third normal form (3NF), Boyce-Codd normal form (BCNF), and fourth normal form (4NF). Let's explore each in detail:\n",
    "\n",
    "### 1. First Normal Form (1NF):\n",
    "\n",
    "**Definition:** A relation is in 1NF if it contains only atomic values, and there are no repeating groups or arrays.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- All attributes must have atomic (indivisible) values.\n",
    "- No repeating groups or arrays are allowed.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "StudentID | Courses\n",
    "-------------------\n",
    "101       | Math, Physics\n",
    "102       | Chemistry\n",
    "```\n",
    "\n",
    "Converting to 1NF:\n",
    "\n",
    "```\n",
    "StudentID | Course\n",
    "-----------------\n",
    "101       | Math\n",
    "101       | Physics\n",
    "102       | Chemistry\n",
    "```\n",
    "\n",
    "### 2. Second Normal Form (2NF):\n",
    "\n",
    "**Definition:** A relation is in 2NF if it is in 1NF and all non-key attributes are fully functionally dependent on the entire primary key.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Be in 1NF.\n",
    "- All non-key attributes should be fully functionally dependent on the primary key.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "OrderID | ProductID | Quantity | Price\n",
    "-------------------------------------\n",
    "1       | A         | 10       | $5\n",
    "1       | B         | 5        | $10\n",
    "2       | A         | 2        | $5\n",
    "```\n",
    "\n",
    "Converting to 2NF:\n",
    "\n",
    "```\n",
    "OrderID | ProductID | Quantity\n",
    "-------------------------------\n",
    "1       | A         | 10\n",
    "1       | B         | 5\n",
    "2       | A         | 2\n",
    "\n",
    "ProductID | Price\n",
    "-----------------\n",
    "A         | $5\n",
    "B         | $10\n",
    "```\n",
    "\n",
    "### 3. Third Normal Form (3NF):\n",
    "\n",
    "**Definition:** A relation is in 3NF if it is in 2NF and all transitive dependencies are removed.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Be in 2NF.\n",
    "- Eliminate transitive dependencies.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "EmployeeID | Department | Location\n",
    "----------------------------------\n",
    "101        | HR         | Building1\n",
    "102        | IT         | Building2\n",
    "```\n",
    "\n",
    "Converting to 3NF:\n",
    "\n",
    "```\n",
    "EmployeeID | Department\n",
    "------------------------\n",
    "101        | HR\n",
    "102        | IT\n",
    "\n",
    "Department | Location\n",
    "---------------------\n",
    "HR         | Building1\n",
    "IT         | Building2\n",
    "```\n",
    "\n",
    "### 4. Boyce-Codd Normal Form (BCNF):\n",
    "\n",
    "**Definition:** A relation is in BCNF if it is in 3NF, and for every non-trivial functional dependency, the determinant is a superkey.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Be in 3NF.\n",
    "- Every non-trivial functional dependency has a superkey as its determinant.\n",
    "\n",
    "### 5. Fourth Normal Form (4NF):\n",
    "\n",
    "**Definition:** A relation is in 4NF if it is in BCNF and multi-valued dependencies are eliminated.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Be in BCNF.\n",
    "- Eliminate multi-valued dependencies.\n",
    "\n",
    "These normal forms guide database designers in structuring data to reduce redundancy and maintain data integrity. The choice of the normalization level depends on the specific requirements of the application and the nature of the data being stored.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
